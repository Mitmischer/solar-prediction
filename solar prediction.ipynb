{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f502f1-ed6e-453a-852b-dd8e605c1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# spectral analysis\n",
    "from scipy import signal\n",
    "from scipy.signal import periodogram as periodogram_f\n",
    "from scipy.fft import fftfreq, fftshift\n",
    "from scipy.fft import fft, ifft, fft2, ifft2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b576bdd-dde5-4cf9-9ccc-ce9ae674c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pandas 2.0, one could use date_format='%Y-%m-%d %H:%M:%S%z', but that's not yet available on Arch Linux\n",
    "solar_ts=pd.read_csv(\"data/energy_charts.csv\", sep=\",\", header=0)#date_format='%Y-%m-%d %H:%M:%S%z')#parse_dates={\"date\": [\"Datum\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solar_ts['Datum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531cc0d3-b869-44b4-9afb-3d1cf5e9fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts['Datum']=pd.to_datetime(solar_ts['Datum'], format='%Y-%m-%dT%H:%M%z', utc=True)\n",
    "solar_ts=solar_ts.set_index(keys=\"Datum\",drop=True)\n",
    "solar_ts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e15e8b-a040-4f18-8f7f-2355d45143df",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfresult = adfuller(solar_ts[2:30000])\n",
    "print(adfresult[0])\n",
    "print(adfresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0a4a7-b3e5-4c89-8a15-f4fe4d04dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data\n",
    "pv = pd.pivot_table(solar_ts, index=solar_ts.index.dayofyear, columns=solar_ts.index.year,\n",
    "                    values='Leistung', aggfunc='sum')\n",
    "pv.plot(cmap=\"Grays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a7dba-0fcc-4035-b780-71ebc35ad6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data\n",
    "pv = pd.pivot_table(solar_ts, index=solar_ts.index.month, columns=solar_ts.index.year,\n",
    "                    values='Leistung', aggfunc='sum')\n",
    "pv.plot(cmap=\"Grays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379a931-52ec-4f08-9637-b0ca016cac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a gap in the data\n",
    "# TODO: Also, there is duplicate data here that pandas duplicated-function will not find...?\n",
    "solar_ts.index[5660:5680]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59dcef-6826-4a52-9727-5109d6e44eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(solar_ts.index.duplicated()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15e79e-23c9-46e0-bc0d-c2548153c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(solar_ts.index[5660:5680]).diff())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8705cd7-e2e5-4167-bcea-a6c0b9f4507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those values need imputation!\n",
    "pd.date_range(solar_ts.index.min(), solar_ts.index.max(), freq='15Min').difference(solar_ts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7261491-58cb-4e1c-96b5-e436eaa70ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This add NaN as value for the missing indices, we can impute this later.\n",
    "solar_ts = solar_ts.resample(\"15Min\").first()\n",
    "# As only a few values need imputation, so the choice of the imputation algorithm does not matter much.\n",
    "solar_ts = solar_ts.interpolate(method=\"time\")\n",
    "# Only now can we infer a frequency.\n",
    "solar_ts=solar_ts.asfreq(pd.infer_freq(solar_ts.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f95b7c-7088-4f79-90a0-9842e9b2111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no duplicated dates, good!\n",
    "# (Although, a bit questionable, see above)\n",
    "np.count_nonzero(solar_ts.index.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ea80c-b9a6-4606-b896-74353461c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts=solar_ts.asfreq(pd.infer_freq(solar_ts.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31293d6a-c751-4c2a-81ab-cb7f3607a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fc4d2-697a-46e1-8334-6f33afebf42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts_series = solar_ts.Leistung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d5c34-5c75-473a-ab94-ff61c9886873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "avg, dev = solar_ts_series.mean(), solar_ts_series.std()\n",
    "solar_ts_series = (solar_ts_series - avg)/dev\n",
    "solar_ts_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fa836-fe31-41ff-a6aa-d316bb924aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove trend (TODO: compare with the approach in the Fourier series video, where they also detrend?)\n",
    "solar_ts_series = solar_ts_series.diff().dropna()\n",
    "solar_ts_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679ad7c-345e-48fc-9494-a16aed55b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider taking another difference: solar_ts_series = solar_ts_series.diff().dropna()\n",
    "# solar_ts_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f9996-9027-4563-b643-44ffec5fd885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove increasing volatility - or (TODO: use a (G)ARCH here).\n",
    "annual_volatility = solar_ts_series.groupby(solar_ts_series.index.year).std()\n",
    "annual_vol_per_day = solar_ts_series.index.map(lambda d: annual_volatility.loc[d.year])\n",
    "solar_ts_series_corrected_variance = solar_ts_series/annual_vol_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d177d73-7d2c-48cc-92d6-f51f05027512",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b68af-4702-4f76-92b6-ddce38ff8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_vol_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1de49e-6a34-4fcc-a69d-bc45b911a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts_series_corrected_variance.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b21932-91ff-4b0a-afa4-149e15168810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ritvik takes monthly means here\n",
    "# why not take dayofyear?\n",
    "monthly_mean = solar_ts_series_corrected_variance.groupby(solar_ts_series_corrected_variance.index.month).mean()\n",
    "monthly_mean_per_day = solar_ts_series_corrected_variance.index.map(lambda d: monthly_mean.loc[d.month])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cfec4-0707-458f-8049-c302c45e8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts_series_corrected_variance= solar_ts_series_corrected_variance - monthly_mean_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e34a52-00b7-47f0-baef-cc93756608e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts_series_corrected_variance.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41265201-fad2-46bc-a641-6029a9103b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only take the first few samples as my RAM explodes otherwise\n",
    "adfresult = adfuller(solar_ts_series_corrected_variance[3:30000])\n",
    "print(adfresult[0])\n",
    "print(adfresult[1])\n",
    "adfresult = adfuller(solar_ts_series_corrected_variance[120000:150000])\n",
    "print(adfresult[0])\n",
    "print(adfresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25a6a4-0f86-4870-8c5c-45759adc4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts_series_corrected_variance=solar_ts_series_corrected_variance[~np.isnan(solar_ts_series_corrected_variance)]\n",
    "solar_ts_series_corrected_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81ebe2-0a6e-4515-82d4-70eefb852582",
   "metadata": {},
   "source": [
    "# some spectral analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b1d88-85b7-4e91-b38b-e2f6d983973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts has period 15 minutes\n",
    "dt = 15*60\n",
    "rate = 1/dt\n",
    "periodogram = np.abs(fft(np.asarray(solar_ts_series_corrected_variance)))**2*dt/(len(solar_ts_series_corrected_variance))\n",
    "frequencies = fftfreq(len(solar_ts_series_corrected_variance), d=1/rate)\n",
    "frequencies\n",
    "plt.plot(fftshift(frequencies), fftshift(periodogram))\n",
    "plt.xlim(-0.00002, 0.0001)\n",
    "plt.ylim(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bef6c-8f1c-401c-8444-9036e00ba307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks (and should be!) similar to the \"manual\" calculation above.\n",
    "# Note that in the manual calculation, we get a symmetric graph. That's to be expected (check out the videos).\n",
    "frequencies, periodogram = periodogram_f(np.asarray(solar_ts_series_corrected_variance), fs=rate, window=\"hamming\")\n",
    "plt.plot(fftshift(frequencies), fftshift(periodogram))\n",
    "plt.xlim(-0.00002, 0.0001)\n",
    "plt.ylim(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7026b-102a-4837-9351-eb35f1071592",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodogram_as_series = pd.Series(fftshift(periodogram), index=fftshift(frequencies))\n",
    "periodogram_as_series = periodogram_as_series[periodogram_as_series.index > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7556d-471b-4638-8f9a-f94ec2960f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert index from frequencies to periods and convert the periods to hours\n",
    "# TODO: is the calculation to hours correct (note that  we already specified the sampling rate during the fft!)?\n",
    "periodogram_as_series.index = (1/periodogram_as_series.index)/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a6874-1b87-4f91-91c1-fce2a4a299f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(periodogram_as_series)\n",
    "plt.xlim(0,100000/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585a74b-8674-4688-8480-0fa50f38f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use SARIMA, => detrend and remove saisonality\n",
    "# Take a look the the residuals\n",
    "# is the model good?\n",
    "# Then, the residuals have no (p)ACF\n",
    "# check QQ - WN has no heavy tails :)\n",
    "# also consider: https://www.youtube.com/watch?v=4zV-ZyQHl7s\n",
    "\n",
    "# TODO: decompose + fit SARIMA model\n",
    "# before: continue with denoising :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9996f-2114-465c-bf86-74eb46a108fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yet another way to calculate the FFT, from the \"denoising\" video.\n",
    "# However, apparently, the signal is now dampened. The frequencies themselves are correct, though\n",
    "# Note how damped the signal appears visually already although the y-scale is really small!\n",
    "n = len(solar_ts_series_corrected_variance)\n",
    "fhat = np.fft.fft(solar_ts_series_corrected_variance, n)\n",
    "PSD = fhat*np.conj(fhat)/n\n",
    "freq = (1/(dt*n))*np.arange(n)\n",
    "L= np.arange(1, np.floor(n/2), dtype=\"int\")\n",
    "plt.plot(freq[L], PSD[L])\n",
    "plt.xlim(-0.00002, 0.0001)\n",
    "plt.ylim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75c6bf-4c2e-4329-ab46-7911dc4fbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now decide on the frequencies to cut off.\n",
    "plt.plot(freq, PSD)\n",
    "plt.ylim(1e-2,1e5)\n",
    "plt.axhline(y=1e0, color=\"r\")\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fce48a-95e8-463b-a1dc-f6458d7de9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only retain frequencies with powers above the red line in the graph above.\n",
    "# I chose it such that almost all high-power frequencies are retained. We get some noisy frequencies in (at the tails) but the majority is filtered.\n",
    "indices = PSD > 1e0\n",
    "# Filter and reconstruct the signal on the retained frequencies (reverse fourier transform)\n",
    "PSDclean = PSD*indices\n",
    "fhat = indices*fhat\n",
    "ffilt = np.fft.ifft(fhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1312d-823b-4b7f-8614-ab59e412a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small sanity check: our new spectrum looks like this: nice, huh?\n",
    "plt.plot(PSDclean)\n",
    "plt.ylim(1e-2,1e5)\n",
    "plt.axhline(y=1e0, color=\"r\")\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6acb68-7925-4894-8a56-b0c4630adfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_ts_series_corrected_variance.plot()\n",
    "plt.ylim(-4,4)\n",
    "pd.Series(ffilt, solar_ts_series_corrected_variance.index).plot()\n",
    "plt.ylim(-4,4)\n",
    "plt.legend([\"original\", \"after fft\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1aba2-df7d-40d8-b5ec-8d480f487b54",
   "metadata": {},
   "source": [
    "# GARCH1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c28540-564e-4889-b443-17eecd36e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solar_ts_series is demeaned but not corrected for increasing variance (which is why we deploy the GARCH model in the first place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67478bba-ad92-4ab3-8c3a-05ab839f29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflux as pf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85cca4-d4c0-4f09-8c1f-05238003f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.DataFrame(np.diff(solar_ts_series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0f3bf-f4fa-4bb2-b5c2-2a1b503d16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca62be-d160-48b5-9530-735c5269da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pf.GARCH(solar_ts_series,p=1,q=1)\n",
    "x = model.fit()\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610292f-9198-4412-860b-901b4e438e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39546a-e158-459c-b3c3-64cb2ec9631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_predict(h=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df253d9-7fdc-4f11-8fc5-0ba00695a399",
   "metadata": {},
   "source": [
    "# Extreme Value Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084672e-2558-4252-8beb-49fd3a23e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyextremes import __version__, get_extremes\n",
    "from pyextremes.plotting import plot_extremes\n",
    "from pyextremes import EVA\n",
    "print(\"pyextremes\", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a841ad-fcef-4a92-85cb-335410cc0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"In order for the analysis results to be meaningful, data needs to be pre-processed by the user. \n",
    "# This may include removal of data gaps, detrending, interpolation, removal of outliers, etc.\"\n",
    "# ==> !! Data needs to be detrended! TODO: Does this also imply constant variance?\n",
    "# I assume yes and take solar_ts_series_corrected_variance as in input\n",
    "# TODO: So what exactly does pyextremes expect from the time series?\n",
    "from pyextremes import EVA\n",
    "\n",
    "solar_ts_series_corrected_variance\n",
    "\n",
    "model = EVA(solar_ts_series_corrected_variance)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1554d6-43ca-41d7-8bf1-63694958fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_extremes(method=\"BM\", block_size=\"365.2425D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4d023-2c28-4b3d-b681-4d12127973b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96daa35-5785-4a9e-ad45-f2d62b1ab872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d473d-d9ef-4dea-94b2-5bf1217ef55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd7c35-a112-4d7d-8704-26bb6c417452",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.get_summary(\n",
    "    return_period=[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000],\n",
    "    alpha=0.95,\n",
    "    n_samples=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49f403-f971-420a-a468-4fe6c2ab8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262360c-8f72-446b-be03-cce5284b1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_diagnostic(alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30240b44-401b-46be-bec0-64293108636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes = get_extremes(\n",
    "    ts=solar_ts_series_corrected_variance,\n",
    "    method=\"BM\",\n",
    "    extremes_type=\"high\",\n",
    "    block_size=\"365.2425D\",\n",
    "    errors=\"raise\",\n",
    "    min_last_block=None,\n",
    ")\n",
    "\n",
    "plot_extremes(\n",
    "    ts=solar_ts_series_corrected_variance,\n",
    "    extremes=extremes,\n",
    "    extremes_method=\"BM\",\n",
    "    extremes_type=\"low\",\n",
    "    block_size=\"365.2425D\",\n",
    ")\n",
    "extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c4bd2-47d0-4c83-b2ee-eb1cc52d7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_extremes(\"POT\", threshold=3, r=\"12H\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84348feb-73e0-4b31-8f50-8797a8cd68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_extremes(show_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f4bc4-ad73-4624-8d29-e2a79de258ab",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddbd41-dd41-411b-abeb-1ddaa2a00c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349da86-b381-4a65-b588-c6359adc8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(solar_ts_series_corrected_variance) * 0.7)\n",
    "ts_train = solar_ts_series_corrected_variance[:train_size]\n",
    "ts_test = solar_ts_series_corrected_variance[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1da0da-6b5f-4bd7-9fc1-c686367568fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ts_train.values.reshape(-1,1)\n",
    "Y_train = ts_test.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c998c-e5dd-4562-8810-32af1ba26d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RBF(length_scale=1)\n",
    "model = GaussianProcessRegressor(kernel=kernel)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5182471-9553-4f2d-800e-8481f34e566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of training data: x_t. Before: window, after: horizon\n",
    "# \"predicting a number\" = regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da094e-d5b3-44af-b39c-3bdd05fe6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evtl. nützlich um Modelle zu vergleichen\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}\n",
    "\n",
    "\n",
    "# consider using the aggregating version (which loses information though - do we really want that?)\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various metrics\n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "  # Account for different sized metrics (for longer horizons, reduce to single number)\n",
    "  if mae.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n",
    "    mae = tf.reduce_mean(mae)\n",
    "    mse = tf.reduce_mean(mse)\n",
    "    rmse = tf.reduce_mean(rmse)\n",
    "    mape = tf.reduce_mean(mape)\n",
    "    mase = tf.reduce_mean(mase)\n",
    "\n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9d055-1cbf-41a0-995b-6d8035be8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1 # predict 1 step at a time\n",
    "WINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon\n",
    "\n",
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=1):\n",
    "  \"\"\"\n",
    "  Creates labels for windowed dataset.\n",
    "\n",
    "  E.g. if horizon=1 (default)\n",
    "  Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n",
    "  \"\"\"\n",
    "  return x[:, :-horizon], x[:, -horizon:]\n",
    "\n",
    "\n",
    "# Test out the window labelling function\n",
    "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\n",
    "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")\n",
    "\n",
    "# Create function to view NumPy arrays as windows\n",
    "def make_windows(x, window_size=7, horizon=1):\n",
    "  \"\"\"\n",
    "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
    "  \"\"\"\n",
    "  # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
    "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "  # print(f\"Window step:\\n {window_step}\")\n",
    "\n",
    "  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
    "  # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
    "\n",
    "  # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
    "  windowed_array = x[window_indexes]\n",
    "\n",
    "  # 4. Get the labelled windows\n",
    "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "\n",
    "  return windows, labels\n",
    "\n",
    "\n",
    "# Consider using tf.keras.preprocessing.timeseries_dataset_from_array() instead!\n",
    "\n",
    "# Make the train/test splits\n",
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "  \"\"\"\n",
    "  Splits matching pairs of windows and labels into train and test splits.\n",
    "  \"\"\"\n",
    "  split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test\n",
    "  train_windows = windows[:split_size]\n",
    "  train_labels = labels[:split_size]\n",
    "  test_windows = windows[split_size:]\n",
    "  test_labels = labels[split_size:]\n",
    "  return train_windows, test_windows, train_labels, test_labels\n",
    "\n",
    "import os\n",
    "\n",
    "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
    "                                            verbose=0, # only output a limited amount of text\n",
    "                                            save_best_only=True) # save only the best model to file\n",
    "\n",
    "def make_preds(model, input_data):\n",
    "  \"\"\"\n",
    "  Uses model to make predictions on input_data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  model: trained model\n",
    "  input_data: windowed input data (same kind of data model was trained on)\n",
    "\n",
    "  Returns model predictions on input_data.\n",
    "  \"\"\"\n",
    "  forecast = model.predict(input_data)\n",
    "  return tf.squeeze(forecast) # return 1D array of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f685cd-6134-4336-91ef-26b9a66eef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional (dense)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seed for as reproducible results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Construct model\n",
    "model_1 = tf.keras.Sequential([\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation\n",
    "], name=\"model_1_dense\") # give the model a name so we can save it\n",
    "\n",
    "# Compile model\n",
    "model_1.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE\n",
    "\n",
    "# Fit model\n",
    "model_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices\n",
    "            y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoint callback to save bes\n",
    "\n",
    "# maybe consider using EarlyStopping instead? \n",
    "# Load in saved best performing model_1 and evaluate on test data\n",
    "model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\")\n",
    "model_1.evaluate(test_windows, test_labels)\n",
    "\n",
    "# Make predictions using model_1 on the test dataset and view the results\n",
    "model_1_preds = make_preds(model_1, test_windows)\n",
    "len(model_1_preds), model_1_preds[:10]\n",
    "\n",
    "# Evaluate preds\n",
    "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape\n",
    "                                 y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166be90d-1a3e-4f66-a7b1-fa559e8791ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2: use different WINDOW_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93f794-5e38-452c-85c8-01eceaa53f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3: horizon > 1 ==> N nodes in the last dense layer, unrolled by make_preds (supposedly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231da4d-b18a-4381-90e2-53b05b5588a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBEATS\n",
    "# Create NBeatsBlock custom layer\n",
    "class NBeatsBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, # the constructor takes all the hyperparameters for the layer\n",
    "               input_size: int,\n",
    "               theta_size: int,\n",
    "               horizon: int,\n",
    "               n_neurons: int,\n",
    "               n_layers: int,\n",
    "               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n",
    "    super().__init__(**kwargs)\n",
    "    self.input_size = input_size\n",
    "    self.theta_size = theta_size\n",
    "    self.horizon = horizon\n",
    "    self.n_neurons = n_neurons\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    # Block contains stack of 4 fully connected layers each has ReLU activation\n",
    "    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n",
    "    # Output of block is a theta layer with linear activation\n",
    "    self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n",
    "\n",
    "  def call(self, inputs): # the call method is what runs when the layer is called\n",
    "    x = inputs\n",
    "    for layer in self.hidden: # pass inputs through each hidden layer\n",
    "      x = layer(x)\n",
    "    theta = self.theta_layer(x)\n",
    "    # Output the backcast and forecast from theta\n",
    "    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n",
    "    return backcast, forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d35fc-f946-4795-bc0e-86a413ab6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294675e-addb-4aa1-bbaa-a2351061ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9075914-c361-4867-a743-feab006e118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_size = int(len(solar_ts_series_corrected_variance) * 0.7)\n",
    "#ts_train = solar_ts_series_corrected_variance[:train_size]\n",
    "#ts_test = solar_ts_series_corrected_variance[train_size:]\n",
    "\n",
    "# Add windowed columns\n",
    "watts_nbeats = solar_ts_series_corrected_variance.to_frame(\"Leistung\")\n",
    "for i in range(WINDOW_SIZE):\n",
    "  watts_nbeats[f\"Leistung+{i+1}\"] = watts_nbeats[\"Leistung\"].shift(periods=i+1)\n",
    "watts_nbeats.dropna().head()\n",
    "\n",
    "X = watts_nbeats.dropna().drop(\"Leistung\", axis=1)\n",
    "y = watts_nbeats.dropna()[\"Leistung\"]\n",
    "\n",
    "# Make train and test sets\n",
    "split_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_size], y[:split_size]\n",
    "X_test, y_test = X[split_size:], y[split_size:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)\n",
    "\n",
    "\n",
    "\n",
    "#X_train = solar_ts_series_corrected_variance.index.to_numpy()\n",
    "#X_train = np.array([x.to_pydatetime() for x in X_train]).astype('datetime64[ns]')\n",
    "#X_test = solar_ts_series_corrected_variance.index.to_numpy()\n",
    "#X_test = np.array([x.to_pydatetime() for x in X_test]).astype('datetime64[ns]')\n",
    "#Y_train = ts_train.values.reshape(-1,1)\n",
    "#Y_test = ts_test.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Turn train and test arrays into tensor Datasets\n",
    "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(Y_train)\n",
    "\n",
    "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "# 2. Combine features & labels\n",
    "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
    "\n",
    "# 3. Batch and prefetch for optimal performance\n",
    "BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55284e-00e1-469e-b3c5-fa4a6d54d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "watts_nbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37a7a4-901f-4427-97f9-89a69668a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09ecac-7e9d-4f8e-bf61-78233ed1f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_object = datetime.fromtimestamp(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83438a9c-7ad8-4250-95a5-8b90210f1891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b442e-fef4-4a25-b4c8-7b9b81f3d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0410b6",
   "metadata": {},
   "source": [
    "# Naive Model and Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Naive Model\n",
    "ts = solar_ts_series_corrected_variance\n",
    "ts = ts.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average model\n",
    "def moving_average(data: pd.DataFrame, window_size: int=4*3, shift_size: int=96):\n",
    "    moving_avg = data.rolling(window=window_size).mean()\n",
    "    shifted_moving_avg = moving_avg.shift(shift_size)\n",
    "    return(shifted_moving_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fc6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Naive Model\n",
    "naive_model = moving_average(ts)\n",
    "\n",
    "test_date_start = '2024-01-01 00:00+00:00'\n",
    "test_ts = ts[test_date_start:]\n",
    "naive_model_print = naive_model[test_date_start:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_ts.index, test_ts, label='Original')\n",
    "plt.plot(naive_model_print.index, naive_model_print, label='Moving average', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Naive Model v.s. Original Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ce79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPredictionModel():\n",
    "    \"\"\"\n",
    "    Time series prediction model implementation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model_name : class\n",
    "            Choice of regressor\n",
    "        model_params : dict\n",
    "            Definition of model specific tuning parameters\n",
    "    \n",
    "    Functions\n",
    "    ----------\n",
    "        train : Train chosen model\n",
    "        forcast : Apply trained model to prediction period and generate forecast DataFrame\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, \n",
    "                 model_params: dict) -> None:\n",
    "        \"\"\"Initialize a new instance of time_series_prediction_model.\"\"\"\n",
    "        self.model = model_name(**model_params) \n",
    "    \n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
    "        \"\"\"Train chosen model.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "    \n",
    "    def forecast(self, X_test: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply trained model to prediction period and generate forecast DataFrame.\"\"\"\n",
    "        self.X_test = X_test\n",
    "        forecast_df = pd.DataFrame(self.model.predict(self.X_test), index=self.X_test.index)\n",
    "        forecast_df.index.name = 'Datum'\n",
    "        return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2858dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data preparation for TimeSeriesPredictionModel\n",
    "\n",
    "data = pd.DataFrame(index=ts.index, columns=['25h_lag', '24h_lag', 'Original'])\n",
    "data['Original'] = ts\n",
    "data['24h_lag'] = ts.shift(96)\n",
    "data['25h_lag'] = ts.shift(100)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f82045",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_start = '2024-01-01 00:00+00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Data train-test split\n",
    "train_df = data[:test_date_start]\n",
    "train_df = train_df.drop(train_df.tail(1).index)\n",
    "X_train = train_df[['25h_lag', '24h_lag']]\n",
    "y_train = train_df[['Original']]\n",
    "\n",
    "test_df = data[test_date_start:]\n",
    "X_test = test_df[['25h_lag', '24h_lag']]\n",
    "y_test = test_df[['Original']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting mit Sliding Window\n",
    "\n",
    "def backtesting(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "                X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
    "                model: TimeSeriesPredictionModel, prediction_step_size: int=96):\n",
    "\n",
    "    # initializing output df\n",
    "    predictions = pd.DataFrame(index=y_test.index, columns=['Original', 'Predictions'])\n",
    "    predictions['Original'] = y_test\n",
    "\n",
    "    for i in range(0, len(X_test)-prediction_step_size, prediction_step_size):\n",
    "        end_idx = i + prediction_step_size\n",
    "        forecast_index= X_test.iloc[i:end_idx].index\n",
    "        \n",
    "        # fit model and predict\n",
    "        model.train(X_train, y_train)\n",
    "        forecast = model.forecast(X_test.iloc[i:end_idx])\n",
    "        predictions.loc[forecast_index, 'Predictions'] = forecast.to_numpy()\n",
    "    \n",
    "        print(f'Finished Forecast for {forecast_index[-1].date()}')\n",
    "\n",
    "        # delete old time window from train data\n",
    "        X_train = X_train.drop(X_train.head(prediction_step_size).index)\n",
    "        y_train = y_train.drop(y_train.head(prediction_step_size).index)\n",
    "\n",
    "        # add next time window to train data\n",
    "        X_train = pd.concat([X_train, X_test.iloc[i:end_idx]])\n",
    "        y_train = pd.concat([y_train, y_test.iloc[i:end_idx]])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be653365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing random forest regressor as instance of TimeSeriesPredictionModel\n",
    "rdnf = TimeSeriesPredictionModel(RandomForestRegressor, {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdnf_pred = backtesting(X_train, y_train, X_test, y_test, rdnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdnf_pred = rdnf_pred.dropna() # ausgehend vom ersten Testzeitraum werden nur vollständige Test-Perioden predicted\n",
    "rdnf_pred.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac37442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return mae, mape, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c607956",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mape, mse, r2 = evaluation(rdnf_pred['Original'], rdnf_pred['Predictions'])\n",
    "\n",
    "print(f'Model: Random Forest \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mape, mse, r2 = evaluation(test_ts, naive_model_print)\n",
    "\n",
    "print(f'Model: Naive Moving Average \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
