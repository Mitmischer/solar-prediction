{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mitmischer/solar-prediction/blob/max/solar%20prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "N979da635zB5",
      "metadata": {
        "id": "N979da635zB5"
      },
      "outputs": [],
      "source": [
        "#!pip install -q condacolab\n",
        "#import condacolab\n",
        "#condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "of9LFfch61WE",
      "metadata": {
        "id": "of9LFfch61WE"
      },
      "outputs": [],
      "source": [
        "#import condacolab\n",
        "#condacolab.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "RmlwZTAJFz4E",
      "metadata": {
        "id": "RmlwZTAJFz4E"
      },
      "outputs": [],
      "source": [
        "#!wget https://files.pythonhosted.org/packages/ad/eb/82206deaf0cc020822465840edf0f9ac6b714be85a9b140be851dceee094/gpforecaster-0.3.142-py3-none-any.whl\n",
        "#!pip install gpforecaster-0.3.142-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "9IcV3WEJ3Hp4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IcV3WEJ3Hp4",
        "outputId": "3c8f5c03-b694-4d1a-a6d8-fa18548e3205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "tZlLj5w-AAYQ",
      "metadata": {
        "id": "tZlLj5w-AAYQ"
      },
      "outputs": [],
      "source": [
        "#!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "#!chmod +x mini.sh\n",
        "#!bash ./mini.sh -b -f -p /usr/local\n",
        "#!conda install -q -y jupyter\n",
        "#!conda install -q -y google-colab -c conda-forge\n",
        "#!python -m ipykernel install --name \"py38\" --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "pBs4PpQBDz45",
      "metadata": {
        "id": "pBs4PpQBDz45"
      },
      "outputs": [],
      "source": [
        "#!pip install gpforecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "_dNCnl_JNkku",
      "metadata": {
        "id": "_dNCnl_JNkku"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/luisroque/hierarchical_gp_forecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "5_82-tzNhDRg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_82-tzNhDRg",
        "outputId": "bc4b17bd-6ce6-423d-8861-cc10b1093ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: GPy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from GPy) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from GPy) (1.16.0)\n",
            "Requirement already satisfied: paramz>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from GPy) (0.9.6)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from GPy) (3.0.10)\n",
            "Requirement already satisfied: scipy<1.12.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from GPy) (1.11.4)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.6->GPy) (4.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install GPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "N49_W-jj-Y_M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N49_W-jj-Y_M",
        "outputId": "1c627363-3e39-42ec-f3b7-989634926652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: darts in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from darts) (0.52)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from darts) (3.7.1)\n",
            "Requirement already satisfied: nfoursid>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.25.2)\n",
            "Requirement already satisfied: pmdarima>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.0.4)\n",
            "Requirement already satisfied: pyod>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from darts) (2.0.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from darts) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from darts) (1.11.4)\n",
            "Requirement already satisfied: shap>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from darts) (0.46.0)\n",
            "Requirement already satisfied: statsforecast>=1.4 in /usr/local/lib/python3.10/dist-packages (from darts) (1.7.5)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from darts) (0.14.2)\n",
            "Requirement already satisfied: tbats>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.1.3)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.10/dist-packages (from darts) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from darts) (4.12.2)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2023.7.0)\n",
            "Requirement already satisfied: xgboost>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.0.3)\n",
            "Requirement already satisfied: pytorch-lightning>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.3.2)\n",
            "Requirement already satisfied: tensorboardX>=2.1 in /usr/local/lib/python3.10/dist-packages (from darts) (2.6.2.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from darts) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays>=0.11.1->darts) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (3.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->darts) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->darts) (2024.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (3.0.10)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (67.7.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from pyod>=0.9.5->darts) (0.58.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (2024.6.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (1.4.0.post0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (0.11.3.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (2024.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->darts) (3.5.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap>=0.40.0->darts) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap>=0.40.0->darts) (2.2.1)\n",
            "Requirement already satisfied: coreforecast>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.0.10)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.9.1)\n",
            "Requirement already satisfied: utilsforecast>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.1.12)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.0->darts) (0.5.6)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.1->darts) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->darts) (12.5.82)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.9.5)\n",
            "Requirement already satisfied: triad>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.9.8)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.14.0->darts) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->darts) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->darts) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (4.0.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (14.0.2)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (2.4.16)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (1.4.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install darts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "eStoDE8fRFvD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eStoDE8fRFvD",
        "outputId": "dc9cc212-e45b-4df1-9e9b-2c23ef978320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gluonts in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.10/dist-packages (from gluonts) (1.25.2)\n",
            "Requirement already satisfied: pandas<3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gluonts) (2.0.3)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts) (2.8.0)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts) (4.66.4)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.0->gluonts) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.0->gluonts) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.0->gluonts) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gluonts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "9H0OGGnQmImZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H0OGGnQmImZ",
        "outputId": "8d39a38a-af54-484c-c76c-26cc498ee61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/RJT1990/pyflux\n",
            "  Cloning https://github.com/RJT1990/pyflux to /tmp/pip-req-build-bw8f9w_b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/RJT1990/pyflux /tmp/pip-req-build-bw8f9w_b\n",
            "  Resolved https://github.com/RJT1990/pyflux to commit 297f2afc2095acd97c12e827dd500e8ea5da0c0f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyflux==0.4.17) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pyflux==0.4.17) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyflux==0.4.17) (1.11.4)\n",
            "Requirement already satisfied: numdifftools in /usr/local/lib/python3.10/dist-packages (from pyflux==0.4.17) (0.9.41)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from pyflux==0.4.17) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pyflux==0.4.17) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyflux==0.4.17) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyflux==0.4.17) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->pyflux==0.4.17) (1.16.0)\n",
            "Requirement already satisfied: armagarch in /usr/local/lib/python3.10/dist-packages (1.0.4)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: statsforecast in /usr/local/lib/python3.10/dist-packages (1.7.5)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.10/dist-packages (2024.6.1)\n",
            "Requirement already satisfied: datasetsforecast in /usr/local/lib/python3.10/dist-packages (0.0.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from statsforecast) (2.2.1)\n",
            "Requirement already satisfied: coreforecast>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.0.10)\n",
            "Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (2.0.3)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.14.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from statsforecast) (4.66.4)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.9.1)\n",
            "Requirement already satisfied: utilsforecast>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.1.12)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from statsforecast) (3.5.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs) (2.13.1)\n",
            "Requirement already satisfied: fsspec==2024.6.1.* in /usr/local/lib/python3.10/dist-packages (from s3fs) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (2.31.0)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (2.0.1)\n",
            "Requirement already satisfied: botocore<1.34.132,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.131)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n",
            "Requirement already satisfied: triad>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast) (0.9.8)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast) (0.2.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.0->statsforecast) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2024.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (2024.6.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->statsforecast) (1.16.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (14.0.2)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast) (2.4.16)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast) (67.7.2)\n",
            "Collecting git+https://github.com/Nixtla/neuralforecast.git@main\n",
            "  Cloning https://github.com/Nixtla/neuralforecast.git (to revision main) to /tmp/pip-req-build-505rp3u1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Nixtla/neuralforecast.git /tmp/pip-req-build-505rp3u1\n",
            "  Resolved https://github.com/Nixtla/neuralforecast.git to commit 0b9fad9d4bea56f4f278687fb03cb424ce25eb6f\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: coreforecast>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (0.0.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (2024.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (2.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (2.3.0+cu121)\n",
            "Requirement already satisfied: pytorch-lightning>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (2.3.2)\n",
            "Requirement already satisfied: ray[tune]>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (2.31.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (3.6.1)\n",
            "Requirement already satisfied: utilsforecast>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from neuralforecast==1.7.3) (0.1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->neuralforecast==1.7.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->neuralforecast==1.7.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->neuralforecast==1.7.3) (2024.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (6.0.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (1.4.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (0.11.3.post0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (2.31.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.2.0->neuralforecast==1.7.3) (14.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->neuralforecast==1.7.3) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->neuralforecast==1.7.3) (12.5.82)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->neuralforecast==1.7.3) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->neuralforecast==1.7.3) (6.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->neuralforecast==1.7.3) (2.0.31)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->neuralforecast==1.7.3) (1.3.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->neuralforecast==1.7.3) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=2.0.0->neuralforecast==1.7.3) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->neuralforecast==1.7.3) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->neuralforecast==1.7.3) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->neuralforecast==1.7.3) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast==1.7.3) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast==1.7.3) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast==1.7.3) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.2.0->neuralforecast==1.7.3) (0.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.2.0->neuralforecast==1.7.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.2.0->neuralforecast==1.7.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.2.0->neuralforecast==1.7.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.2.0->neuralforecast==1.7.3) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->neuralforecast==1.7.3) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->neuralforecast==1.7.3) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->neuralforecast==1.7.3) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->neuralforecast==1.7.3) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/RJT1990/pyflux\n",
        "!pip install armagarch\n",
        "!pip install statsmodels\n",
        "!pip install statsforecast s3fs datasetsforecast\n",
        "!pip install git+https://github.com/Nixtla/neuralforecast.git@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "36f502f1-ed6e-453a-852b-dd8e605c1799",
      "metadata": {
        "id": "36f502f1-ed6e-453a-852b-dd8e605c1799"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import armagarch as ag\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# spectral analysis\n",
        "from scipy import signal\n",
        "from scipy.signal import periodogram as periodogram_f\n",
        "from scipy.fft import fftfreq, fftshift\n",
        "from scipy.fft import fft, ifft, fft2, ifft2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "9b576bdd-dde5-4cf9-9ccc-ce9ae674c14b",
      "metadata": {
        "id": "9b576bdd-dde5-4cf9-9ccc-ce9ae674c14b"
      },
      "outputs": [],
      "source": [
        "# with pandas 2.0, one could use date_format='%Y-%m-%d %H:%M:%S%z', but that's not yet available on Arch Linux\n",
        "solar_ts=pd.read_csv(\"/content/drive/MyDrive/energy_charts.csv\", sep=\",\", header=0)#date_format='%Y-%m-%d %H:%M:%S%z')#parse_dates={\"date\": [\"Datum\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531cc0d3-b869-44b4-9afb-3d1cf5e9fe20",
      "metadata": {
        "id": "531cc0d3-b869-44b4-9afb-3d1cf5e9fe20"
      },
      "outputs": [],
      "source": [
        "solar_ts['Datum']=pd.to_datetime(solar_ts['Datum'], format='%Y-%m-%dT%H:%M%z', utc=True)\n",
        "solar_ts=solar_ts.set_index(keys=\"Datum\",drop=True)\n",
        "solar_ts.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e15e8b-a040-4f18-8f7f-2355d45143df",
      "metadata": {
        "id": "80e15e8b-a040-4f18-8f7f-2355d45143df"
      },
      "outputs": [],
      "source": [
        "adfresult = adfuller(solar_ts[2:30000])\n",
        "print(adfresult[0])\n",
        "print(adfresult[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a0a4a7-b3e5-4c89-8a15-f4fe4d04dcb0",
      "metadata": {
        "id": "69a0a4a7-b3e5-4c89-8a15-f4fe4d04dcb0"
      },
      "outputs": [],
      "source": [
        "# see https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data\n",
        "pv = pd.pivot_table(solar_ts, index=solar_ts.index.dayofyear, columns=solar_ts.index.year,\n",
        "                    values='Leistung', aggfunc='sum')\n",
        "pv.plot(cmap=\"Greys\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0a7dba-0fcc-4035-b780-71ebc35ad6b9",
      "metadata": {
        "id": "dd0a7dba-0fcc-4035-b780-71ebc35ad6b9"
      },
      "outputs": [],
      "source": [
        "# see https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data\n",
        "pv = pd.pivot_table(solar_ts, index=solar_ts.index.month, columns=solar_ts.index.year,\n",
        "                    values='Leistung', aggfunc='sum')\n",
        "pv.plot(cmap=\"Greys\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6379a931-52ec-4f08-9637-b0ca016cac9a",
      "metadata": {
        "id": "6379a931-52ec-4f08-9637-b0ca016cac9a"
      },
      "outputs": [],
      "source": [
        "# An example of a gap in the data\n",
        "# TODO: Also, there is duplicate data here that pandas duplicated-function will not find...?\n",
        "solar_ts.index[5660:5680]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd59dcef-6826-4a52-9727-5109d6e44eba",
      "metadata": {
        "id": "cd59dcef-6826-4a52-9727-5109d6e44eba"
      },
      "outputs": [],
      "source": [
        "pd.Series(solar_ts.index.duplicated()).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf15e79e-23c9-46e0-bc0d-c2548153c3ef",
      "metadata": {
        "id": "cf15e79e-23c9-46e0-bc0d-c2548153c3ef"
      },
      "outputs": [],
      "source": [
        "(pd.Series(solar_ts.index[5660:5680]).diff())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8705cd7-e2e5-4167-bcea-a6c0b9f4507e",
      "metadata": {
        "id": "a8705cd7-e2e5-4167-bcea-a6c0b9f4507e"
      },
      "outputs": [],
      "source": [
        "# Those values need imputation!\n",
        "pd.date_range(solar_ts.index.min(), solar_ts.index.max(), freq='15Min').difference(solar_ts.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7261491-58cb-4e1c-96b5-e436eaa70ca4",
      "metadata": {
        "id": "a7261491-58cb-4e1c-96b5-e436eaa70ca4"
      },
      "outputs": [],
      "source": [
        "# This add NaN as value for the missing indices, we can impute this later.\n",
        "solar_ts = solar_ts.resample(\"15Min\").first()\n",
        "# As only a few values need imputation, so the choice of the imputation algorithm does not matter much.\n",
        "solar_ts = solar_ts.interpolate(method=\"time\")\n",
        "# Only now can we infer a frequency.\n",
        "solar_ts=solar_ts.asfreq(pd.infer_freq(solar_ts.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f95b7c-7088-4f79-90a0-9842e9b2111a",
      "metadata": {
        "id": "73f95b7c-7088-4f79-90a0-9842e9b2111a"
      },
      "outputs": [],
      "source": [
        "# There are no duplicated dates, good!\n",
        "# (Although, a bit questionable, see above)\n",
        "np.count_nonzero(solar_ts.index.duplicated())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98ea80c-b9a6-4606-b896-74353461c4af",
      "metadata": {
        "id": "e98ea80c-b9a6-4606-b896-74353461c4af"
      },
      "outputs": [],
      "source": [
        "solar_ts=solar_ts.asfreq(pd.infer_freq(solar_ts.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31293d6a-c751-4c2a-81ab-cb7f3607a482",
      "metadata": {
        "id": "31293d6a-c751-4c2a-81ab-cb7f3607a482"
      },
      "outputs": [],
      "source": [
        "solar_ts.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1fc4d2-697a-46e1-8334-6f33afebf42f",
      "metadata": {
        "id": "9e1fc4d2-697a-46e1-8334-6f33afebf42f"
      },
      "outputs": [],
      "source": [
        "solar_ts_series = solar_ts.Leistung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "952d5c34-5c75-473a-ab94-ff61c9886873",
      "metadata": {
        "id": "952d5c34-5c75-473a-ab94-ff61c9886873"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "train_size = int(len(solar_ts_series) * 0.7)\n",
        "ts_train = solar_ts_series[:train_size]\n",
        "ts_test = solar_ts_series[train_size:]\n",
        "\n",
        "# we cannot normalize over the test data as we don't have that yet!\n",
        "avg, dev = ts_train.mean(), ts_train.std()\n",
        "solar_ts_series = (solar_ts_series - avg)/dev\n",
        "solar_ts_series.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d21fa836-fe31-41ff-a6aa-d316bb924aa4",
      "metadata": {
        "id": "d21fa836-fe31-41ff-a6aa-d316bb924aa4"
      },
      "outputs": [],
      "source": [
        "# Remove trend (TODO: compare with the approach in the Fourier series video, where they also detrend?)\n",
        "solar_ts_series = solar_ts_series.diff().dropna()\n",
        "solar_ts_series.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c679ad7c-345e-48fc-9494-a16aed55b2fd",
      "metadata": {
        "id": "c679ad7c-345e-48fc-9494-a16aed55b2fd"
      },
      "outputs": [],
      "source": [
        "# Consider taking another difference: solar_ts_series = solar_ts_series.diff().dropna()\n",
        "# solar_ts_series.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4f9996-9027-4563-b643-44ffec5fd885",
      "metadata": {
        "id": "0b4f9996-9027-4563-b643-44ffec5fd885"
      },
      "outputs": [],
      "source": [
        "# remove increasing volatility - or (TODO: use a (G)ARCH here).\n",
        "annual_volatility = solar_ts_series.groupby(solar_ts_series.index.year).std()\n",
        "annual_vol_per_day = solar_ts_series.index.map(lambda d: annual_volatility.loc[d.year])\n",
        "solar_ts_series_corrected_variance = solar_ts_series/annual_vol_per_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d177d73-7d2c-48cc-92d6-f51f05027512",
      "metadata": {
        "id": "4d177d73-7d2c-48cc-92d6-f51f05027512"
      },
      "outputs": [],
      "source": [
        "annual_volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d04b68af-4702-4f76-92b6-ddce38ff8734",
      "metadata": {
        "id": "d04b68af-4702-4f76-92b6-ddce38ff8734"
      },
      "outputs": [],
      "source": [
        "annual_vol_per_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1de49e-6a34-4fcc-a69d-bc45b911a217",
      "metadata": {
        "id": "4f1de49e-6a34-4fcc-a69d-bc45b911a217"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  solar_ts_series_corrected_variance.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b21932-91ff-4b0a-afa4-149e15168810",
      "metadata": {
        "id": "a0b21932-91ff-4b0a-afa4-149e15168810"
      },
      "outputs": [],
      "source": [
        "# ritvik takes monthly means here\n",
        "# why not take dayofyear?\n",
        "monthly_mean = solar_ts_series_corrected_variance.groupby(solar_ts_series_corrected_variance.index.month).mean()\n",
        "monthly_mean_per_day = solar_ts_series_corrected_variance.index.map(lambda d: monthly_mean.loc[d.month])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895cfec4-0707-458f-8049-c302c45e8b6d",
      "metadata": {
        "id": "895cfec4-0707-458f-8049-c302c45e8b6d"
      },
      "outputs": [],
      "source": [
        "solar_ts_series_corrected_variance= solar_ts_series_corrected_variance - monthly_mean_per_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e34a52-00b7-47f0-baef-cc93756608e3",
      "metadata": {
        "id": "a5e34a52-00b7-47f0-baef-cc93756608e3"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  solar_ts_series_corrected_variance.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41265201-fad2-46bc-a641-6029a9103b7e",
      "metadata": {
        "id": "41265201-fad2-46bc-a641-6029a9103b7e"
      },
      "outputs": [],
      "source": [
        "# we only take the first few samples as my RAM explodes otherwise\n",
        "adfresult = adfuller(solar_ts_series_corrected_variance[3:30000])\n",
        "print(adfresult[0])\n",
        "print(adfresult[1])\n",
        "adfresult = adfuller(solar_ts_series_corrected_variance[120000:150000])\n",
        "print(adfresult[0])\n",
        "print(adfresult[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f25a6a4-0f86-4870-8c5c-45759adc4573",
      "metadata": {
        "id": "9f25a6a4-0f86-4870-8c5c-45759adc4573"
      },
      "outputs": [],
      "source": [
        "solar_ts_series_corrected_variance=solar_ts_series_corrected_variance[~np.isnan(solar_ts_series_corrected_variance)]\n",
        "solar_ts_series_corrected_variance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c81ebe2-0a6e-4515-82d4-70eefb852582",
      "metadata": {
        "id": "3c81ebe2-0a6e-4515-82d4-70eefb852582"
      },
      "source": [
        "# some spectral analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e0b1d88-85b7-4e91-b38b-e2f6d983973e",
      "metadata": {
        "id": "9e0b1d88-85b7-4e91-b38b-e2f6d983973e"
      },
      "outputs": [],
      "source": [
        "# ts has period 15 minutes\n",
        "kwitt\n",
        "dt = 15*60\n",
        "rate = 1/dt\n",
        "periodogram = np.abs(fft(np.asarray(solar_ts_series_corrected_variance)))**2*dt/(len(solar_ts_series_corrected_variance))\n",
        "frequencies = fftfreq(len(solar_ts_series_corrected_variance), d=1/rate)\n",
        "frequencies\n",
        "plt.plot(fftshift(frequencies), fftshift(periodogram))\n",
        "plt.xlim(-0.00002, 0.0001)\n",
        "plt.ylim(0, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333bef6c-8f1c-401c-8444-9036e00ba307",
      "metadata": {
        "id": "333bef6c-8f1c-401c-8444-9036e00ba307"
      },
      "outputs": [],
      "source": [
        "# looks (and should be!) similar to the \"manual\" calculation above.\n",
        "# Note that in the manual calculation, we get a symmetric graph. That's to be expected (check out the videos).\n",
        "frequencies, periodogram = periodogram_f(np.asarray(solar_ts_series_corrected_variance), fs=rate, window=\"hamming\")\n",
        "plt.plot(fftshift(frequencies), fftshift(periodogram))\n",
        "plt.xlim(-0.00002, 0.0001)\n",
        "plt.ylim(0, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd7026b-102a-4837-9351-eb35f1071592",
      "metadata": {
        "id": "cfd7026b-102a-4837-9351-eb35f1071592"
      },
      "outputs": [],
      "source": [
        "periodogram_as_series = pd.Series(fftshift(periodogram), index=fftshift(frequencies))\n",
        "periodogram_as_series = periodogram_as_series[periodogram_as_series.index > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c7556d-471b-4638-8f9a-f94ec2960f40",
      "metadata": {
        "id": "14c7556d-471b-4638-8f9a-f94ec2960f40"
      },
      "outputs": [],
      "source": [
        "# convert index from frequencies to periods and convert the periods to hours\n",
        "# TODO: is the calculation to hours correct (note that  we already specified the sampling rate during the fft!)?\n",
        "periodogram_as_series.index = (1/periodogram_as_series.index)/3600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a97a6874-1b87-4f91-91c1-fce2a4a299f3",
      "metadata": {
        "id": "a97a6874-1b87-4f91-91c1-fce2a4a299f3"
      },
      "outputs": [],
      "source": [
        "plt.plot(periodogram_as_series)\n",
        "plt.xlim(0,100000/3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8585a74b-8674-4688-8480-0fa50f38f66d",
      "metadata": {
        "id": "8585a74b-8674-4688-8480-0fa50f38f66d"
      },
      "outputs": [],
      "source": [
        "# TODO: use SARIMA, => detrend and remove saisonality\n",
        "# Take a look the the residuals\n",
        "# is the model good?\n",
        "# Then, the residuals have no (p)ACF\n",
        "# check QQ - WN has no heavy tails :)\n",
        "# also consider: https://www.youtube.com/watch?v=4zV-ZyQHl7s\n",
        "\n",
        "# TODO: decompose + fit SARIMA model\n",
        "# before: continue with denoising :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d9996f-2114-465c-bf86-74eb46a108fb",
      "metadata": {
        "id": "10d9996f-2114-465c-bf86-74eb46a108fb"
      },
      "outputs": [],
      "source": [
        "# yet another way to calculate the FFT, from the \"denoising\" video.\n",
        "# However, apparently, the signal is now dampened. The frequencies themselves are correct, though\n",
        "# Note how damped the signal appears visually already although the y-scale is really small!\n",
        "n = len(solar_ts_series_corrected_variance)\n",
        "fhat = np.fft.fft(solar_ts_series_corrected_variance, n)\n",
        "PSD = fhat*np.conj(fhat)/n\n",
        "freq = (1/(dt*n))*np.arange(n)\n",
        "L= np.arange(1, np.floor(n/2), dtype=\"int\")\n",
        "plt.plot(freq[L], PSD[L])\n",
        "plt.xlim(-0.00002, 0.0001)\n",
        "plt.ylim(0, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b75c6bf-4c2e-4329-ab46-7911dc4fbce4",
      "metadata": {
        "id": "0b75c6bf-4c2e-4329-ab46-7911dc4fbce4"
      },
      "outputs": [],
      "source": [
        "# Now decide on the frequencies to cut off.\n",
        "plt.plot(freq, PSD)\n",
        "plt.ylim(1e-2,1e5)\n",
        "plt.axhline(y=1e0, color=\"r\")\n",
        "plt.semilogy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fce48a-95e8-463b-a1dc-f6458d7de9e8",
      "metadata": {
        "id": "d4fce48a-95e8-463b-a1dc-f6458d7de9e8"
      },
      "outputs": [],
      "source": [
        "# We'll only retain frequencies with powers above the red line in the graph above.\n",
        "# I chose it such that almost all high-power frequencies are retained. We get some noisy frequencies in (at the tails) but the majority is filtered.\n",
        "indices = PSD > 1e0\n",
        "# Filter and reconstruct the signal on the retained frequencies (reverse fourier transform)\n",
        "PSDclean = PSD*indices\n",
        "fhat = indices*fhat\n",
        "ffilt = np.fft.ifft(fhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad1312d-823b-4b7f-8614-ab59e412a65a",
      "metadata": {
        "id": "6ad1312d-823b-4b7f-8614-ab59e412a65a"
      },
      "outputs": [],
      "source": [
        "# small sanity check: our new spectrum looks like this: nice, huh?\n",
        "plt.plot(PSDclean)\n",
        "plt.ylim(1e-2,1e5)\n",
        "plt.axhline(y=1e0, color=\"r\")\n",
        "plt.semilogy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6acb68-7925-4894-8a56-b0c4630adfb1",
      "metadata": {
        "id": "aa6acb68-7925-4894-8a56-b0c4630adfb1"
      },
      "outputs": [],
      "source": [
        "solar_ts_series_corrected_variance.plot()\n",
        "plt.ylim(-4,4)\n",
        "pd.Series(ffilt, solar_ts_series_corrected_variance.index).plot()\n",
        "plt.ylim(-4,4)\n",
        "plt.legend([\"original\", \"after fft\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tz7prQAvEMDe",
      "metadata": {
        "id": "Tz7prQAvEMDe"
      },
      "source": [
        "# FT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JWpqiychLJa6",
      "metadata": {
        "id": "JWpqiychLJa6"
      },
      "outputs": [],
      "source": [
        "!pip install darts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mIeW_AchEMw2",
      "metadata": {
        "id": "mIeW_AchEMw2"
      },
      "outputs": [],
      "source": [
        "# Evtl. erwgen eins der GP-Repos zu porten? z.B. ABCDflow?\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.models import FFT, AutoARIMA, ExponentialSmoothing, Theta\n",
        "from darts.metrics import mae\n",
        "from darts.utils.missing_values import fill_missing_values\n",
        "from darts.datasets import TemperatureDataset, AirPassengersDataset, EnergyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yj8QO-LLIACr",
      "metadata": {
        "id": "yj8QO-LLIACr"
      },
      "outputs": [],
      "source": [
        "from darts import TimeSeries\n",
        "\n",
        "train_size = int(len(solar_ts_series_corrected_variance) * 0.75)\n",
        "ts_train = solar_ts_series_corrected_variance[:train_size]\n",
        "ts_test = solar_ts_series_corrected_variance[train_size:]\n",
        "\n",
        "darts_ts_train = TimeSeries.from_series(ts_train)\n",
        "darts_ts_test = TimeSeries.from_series(ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_cNs1jnKNiFJ",
      "metadata": {
        "id": "_cNs1jnKNiFJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(19,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Io_i7CHS0Yli",
      "metadata": {
        "id": "Io_i7CHS0Yli"
      },
      "outputs": [],
      "source": [
        "def do_fourier(model, darts_ts_train, darts_ts_test):\n",
        "  model.fit(darts_ts_train)\n",
        "  pred_val = model.predict(len(darts_ts_test))\n",
        "  plt.figure(figsize=(24,4))\n",
        "  darts_ts_train.plot(label=\"train\")\n",
        "  darts_ts_test.plot(label=\"val\")\n",
        "  print(\"MAE:\", mae(pred_val, darts_ts_test))\n",
        "  return pred_val.plot(label=\"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JngxR4rOIUva",
      "metadata": {
        "id": "JngxR4rOIUva"
      },
      "outputs": [],
      "source": [
        "# evtl. Abfolge ganz interessant:\n",
        "# schlecht: model = FFT(required_matches=set(), nr_freqs_to_keep=None)\n",
        "# todo: mit dem Parameter nr_freqs_to_keep rumspielen, der ist sehr wichtig!!\n",
        "model = FFT(required_matches=set(), nr_freqs_to_keep=None)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JKiXzXJqK8UY",
      "metadata": {
        "id": "JKiXzXJqK8UY"
      },
      "outputs": [],
      "source": [
        "model = FFT(required_matches={\"month\"}, nr_freqs_to_keep=None)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5yja2Hx118Xi",
      "metadata": {
        "id": "5yja2Hx118Xi"
      },
      "outputs": [],
      "source": [
        "model = FFT(nr_freqs_to_keep=None)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thgxuayr2BTT",
      "metadata": {
        "id": "thgxuayr2BTT"
      },
      "outputs": [],
      "source": [
        "model = FFT(nr_freqs_to_keep=8000)\n",
        "do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wWoljzoiOfjE",
      "metadata": {
        "id": "wWoljzoiOfjE"
      },
      "outputs": [],
      "source": [
        "model = FFT(nr_freqs_to_keep=8000)\n",
        "model.gridsearch(\n",
        "  parameters={\n",
        "      \"nr_freqs_to_keep\": [1000, 5000, 10000, 15000, 20000]\n",
        "  },\n",
        "  series=darts_ts_train,\n",
        "  val_series=darts_ts_test,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "#do_fourier(model, darts_ts_train, darts_ts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lGMbsLxZ2k8O",
      "metadata": {
        "id": "lGMbsLxZ2k8O"
      },
      "outputs": [],
      "source": [
        "# now on the data with original (=increasing) volatility\n",
        "\n",
        "ts_train_orig = solar_ts_series[:train_size]\n",
        "ts_test_orig = solar_ts_series[train_size:]\n",
        "\n",
        "darts_ts_train_orig = TimeSeries.from_series(ts_train_orig)\n",
        "darts_ts_test_orig = TimeSeries.from_series(ts_test_orig)\n",
        "ts_cont_pred = TimeSeries.from_series(pd.Series((3*list(ts_train_orig[-365*24*4:].values))[0:len(ts_test_orig.index)], index=ts_test_orig.index))\n",
        "\n",
        "\n",
        "model = FFT(nr_freqs_to_keep=None)\n",
        "do_fourier(model, darts_ts_train_orig, darts_ts_test_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pie-xFts6JSd",
      "metadata": {
        "id": "Pie-xFts6JSd"
      },
      "outputs": [],
      "source": [
        "model = FFT(nr_freqs_to_keep=None, trend=\"poly\")\n",
        "do_fourier(model, darts_ts_train_orig, darts_ts_test_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sukUNZEZ7aNV",
      "metadata": {
        "id": "sukUNZEZ7aNV"
      },
      "outputs": [],
      "source": [
        "# Evtl. als spannende zulssige baselines:\n",
        "# das Jahr vorher?\n",
        "# der Tag vorher?\n",
        "\n",
        "darts_ts_train_orig.plot(label=\"train\")\n",
        "darts_ts_test_orig.plot(label=\"val\")\n",
        "print(\"MAE:\", mae(ts_cont_pred, darts_ts_test_orig))\n",
        "ts_cont_pred.plot(label=\"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qUrSoyvoHg3z",
      "metadata": {
        "id": "qUrSoyvoHg3z"
      },
      "outputs": [],
      "source": [
        "ts_cont_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5de1aba2-df7d-40d8-b5ec-8d480f487b54",
      "metadata": {
        "id": "5de1aba2-df7d-40d8-b5ec-8d480f487b54"
      },
      "source": [
        "# GARCH1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6c28540-564e-4889-b443-17eecd36e5c2",
      "metadata": {
        "id": "d6c28540-564e-4889-b443-17eecd36e5c2"
      },
      "outputs": [],
      "source": [
        "# solar_ts_series is demeaned but not corrected for increasing variance (which is why we deploy the GARCH model in the first place)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67478bba-ad92-4ab3-8c3a-05ab839f29d3",
      "metadata": {
        "id": "67478bba-ad92-4ab3-8c3a-05ab839f29d3"
      },
      "outputs": [],
      "source": [
        "import pyflux as pf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cQ-pnuMjnOe6",
      "metadata": {
        "id": "cQ-pnuMjnOe6"
      },
      "outputs": [],
      "source": [
        "solar_ts_series\n",
        "solar_ts_series = pd.Series(solar_ts_series)\n",
        "solar_ts_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4ca62be-d160-48b5-9530-735c5269da55",
      "metadata": {
        "id": "d4ca62be-d160-48b5-9530-735c5269da55"
      },
      "outputs": [],
      "source": [
        "model = pf.GARCH(solar_ts_series.reset_index(drop=True).values[0:3000],p=1,q=1)\n",
        "x = model.fit()\n",
        "x.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f610292f-9198-4412-860b-901b4e438e33",
      "metadata": {
        "id": "f610292f-9198-4412-860b-901b4e438e33"
      },
      "outputs": [],
      "source": [
        "model.plot_fit(figsize=(19,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6D53w-zbs3lb",
      "metadata": {
        "id": "6D53w-zbs3lb"
      },
      "outputs": [],
      "source": [
        "np.abs(solar_ts_series).plot(figsize=(14,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lmir7ZSXq2dM",
      "metadata": {
        "id": "lmir7ZSXq2dM"
      },
      "outputs": [],
      "source": [
        "model.plot_predict(h=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bzoWRNGKxxvB",
      "metadata": {
        "id": "bzoWRNGKxxvB"
      },
      "outputs": [],
      "source": [
        "model.plot_predict_is(h=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5mS19tStn0Z",
      "metadata": {
        "id": "c5mS19tStn0Z"
      },
      "outputs": [],
      "source": [
        "# Conclusion: Excellent fit (almost \"too perfect\"), but high uncertainty. Not fit for predictions, really..\n",
        "# TODO: Evtl. noch ein Plot mit der echten prediction?\n",
        "# Evtl. in Ordnung fr nur eine weitere Stunde, also mit backtesting?\n",
        "# Knnen wir das auch fr einene greren horizon?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ofxRcFGuxvUi",
      "metadata": {
        "id": "ofxRcFGuxvUi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dg2DLDikzFgT",
      "metadata": {
        "id": "dg2DLDikzFgT"
      },
      "source": [
        "# GARCH2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51P4WjiYzHQ8",
      "metadata": {
        "id": "51P4WjiYzHQ8"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm # Import statsmodels.api\n",
        "\n",
        "meanMdl = ag.ARMA(order = {'AR':1,'MA':1})\n",
        "volMdl = ag.garch(order = {'p':1,'q':1})\n",
        "distMdl = ag.normalDist()\n",
        "\n",
        "# create a model\n",
        "# Dont want to wait!\n",
        "model = ag.empModel(solar_ts_series[1:1000].to_frame(), meanMdl, volMdl, distMdl)\n",
        "# Fit model\n",
        "model.fit()\n",
        "\n",
        "pred = model.predict(nsteps=4*24)\n",
        "mean_pred, var_pred = pred\n",
        "\n",
        "\n",
        "# Plot the actual data and the forecast for the last 100 observations\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the actual data for the last 100 observations\n",
        "plt.plot(solar_ts_series[-100:], label='Actual Data')\n",
        "\n",
        "# Plot the forecasted mean, starting from the last actual value\n",
        "last_observation_index = solar_ts_series.index[-1]\n",
        "forecast_index = pd.date_range(start=last_observation_index, periods=len(mean_pred) + 1, freq=\"15min\")[1:]\n",
        "plt.plot(forecast_index[-100:], mean_pred[-100:], label='Forecasted Mean', color='red')\n",
        "\n",
        "# Fill between upper and lower bounds of the forecasted variance\n",
        "plt.fill_between(forecast_index[-100:],\n",
        "                 mean_pred[-100:] - np.sqrt(var_pred[-100:]),\n",
        "                 mean_pred[-100:] + np.sqrt(var_pred[-100:]),\n",
        "                 color='red', alpha=0.2, label='Forecasted Variance')\n",
        "\n",
        "plt.title('ARMA-GARCH Forecast (Last 100 Observations)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Returns')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kywfuwuJHkso",
      "metadata": {
        "id": "kywfuwuJHkso"
      },
      "source": [
        "# AD1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H3PbUzG9HnUr",
      "metadata": {
        "id": "H3PbUzG9HnUr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r5Z5Am-eIOPE",
      "metadata": {
        "id": "r5Z5Am-eIOPE"
      },
      "outputs": [],
      "source": [
        "# Normalize and save the mean and std we get,\n",
        "# for normalizing test data.\n",
        "\n",
        "TIME_STEPS = 288\n",
        "\n",
        "# Generated training sequences for use in the model.\n",
        "def create_sequences(values, time_steps):\n",
        "    output = []\n",
        "    for i in range(len(values) - time_steps + 1):\n",
        "        output.append(values[i : (i + time_steps)])\n",
        "    return np.stack(output)\n",
        "\n",
        "\n",
        "def AE_anomaly_detection(x_train, x_test, time_steps):\n",
        "    simple = True\n",
        "    training_mean = x_train.mean()\n",
        "    training_std = x_train.std()\n",
        "    df_training_value = (x_train - training_mean) / training_std\n",
        "    print(\"Number of training samples:\", len(df_training_value))\n",
        "\n",
        "    x_train = create_sequences(df_training_value.values, time_steps)\n",
        "    print(\"Training input shape: \", x_train.shape)\n",
        "\n",
        "    if simple:\n",
        "      model = keras.Sequential(\n",
        "          [\n",
        "              layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
        "              layers.Conv1D(\n",
        "                  filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "              ),\n",
        "              layers.Dropout(rate=0.2),\n",
        "              layers.Conv1D(\n",
        "                  filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "              ),\n",
        "              layers.Conv1DTranspose(\n",
        "                  filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "              ),\n",
        "              layers.Dropout(rate=0.2),\n",
        "              layers.Conv1DTranspose(\n",
        "                  filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
        "              ),\n",
        "              layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
        "          ]\n",
        "      )\n",
        "    else:\n",
        "      model = keras.Sequential()\n",
        "      model.add(layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "      model.add(layers.Dropout(rate=0.2))\n",
        "      model.add(layers.RepeatVector(x_train.shape[1]))\n",
        "      model.add(layers.LSTM(128, return_sequences=True))\n",
        "      model.add(layers.Dropout(rate=0.2))\n",
        "      model.add(layers.TimeDistributed(Dense(x_train.shape[2])))\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "    history = model.fit(\n",
        "    x_train,\n",
        "    x_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
        "       ],\n",
        "    )\n",
        "    # Get train MAE loss.\n",
        "    x_train_pred = model.predict(x_train)\n",
        "    train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
        "\n",
        "    plt.hist(train_mae_loss, bins=50)\n",
        "    plt.xlabel(\"Train MAE loss\")\n",
        "    plt.ylabel(\"No of samples\")\n",
        "    plt.show()\n",
        "\n",
        "    # Get reconstruction loss threshold.\n",
        "    threshold = np.max(train_mae_loss)*0.9\n",
        "    print(\"Reconstruction error threshold: \", threshold)\n",
        "\n",
        "\n",
        "##### test ...\n",
        "\n",
        "    test_mean = x_test.mean()\n",
        "    test_std = x_test.std()\n",
        "    ####### prepare the test data\n",
        "    df_test_value = (x_test - test_mean) / test_std\n",
        "    #fig, ax = plt.subplots()\n",
        "    #df_test_value.plot(legend=False, ax=ax)\n",
        "    #plt.show()\n",
        "\n",
        "    # Create sequences from test values.\n",
        "    x_test = create_sequences(df_test_value.values, time_steps)\n",
        "    print(\"Test input shape: \", x_test.shape)\n",
        "\n",
        "    # Get test MAE loss.\n",
        "    # das hier mssen wir spter auf all unsere samples geben, um die\n",
        "    #  Anomalien zu entdecken.\n",
        "    x_test_pred = model.predict(x_test)\n",
        "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
        "    test_mae_loss = test_mae_loss.reshape((-1))\n",
        "\n",
        "    plt.hist(test_mae_loss, bins=50)\n",
        "    plt.xlabel(\"test MAE loss\")\n",
        "    plt.ylabel(\"No of samples\")\n",
        "    plt.show()\n",
        "\n",
        "    # Detect all the samples which are anomalies.\n",
        "    anomalies = test_mae_loss > threshold\n",
        "    print(anomalies)\n",
        "    print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "    #print(\"Indices of anomaly samples: \", np.where(anomalies))\n",
        "    return anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebC18C5wIQBQ",
      "metadata": {
        "id": "ebC18C5wIQBQ"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 288\n",
        "\n",
        "df_daily_jumpsup = solar_ts_series\n",
        "\n",
        "##### plot anomalies\n",
        "anomalies = AE_anomaly_detection(solar_ts_series.iloc[1:1000].to_frame(), solar_ts_series.iloc[1001:60000].to_frame(), time_steps=TIME_STEPS)\n",
        "\n",
        "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
        "test_mean = df_daily_jumpsup.mean()\n",
        "test_std = df_daily_jumpsup.std()\n",
        "df_test_value = (df_daily_jumpsup - test_mean) / test_std\n",
        "\n",
        "anomalous_data_indices = []\n",
        "inconclusive_indices = []\n",
        "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
        "    sample_anomalies = anomalies[data_idx - TIME_STEPS + 1 : data_idx]\n",
        "    if len(sample_anomalies) == 0:\n",
        "        inconclusive_indices.append(data_idx)\n",
        "    elif np.mean(sample_anomalies) > 0.9:\n",
        "        anomalous_data_indices.append(data_idx)\n",
        "abnormal_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n",
        "inconclusive_subset = df_daily_jumpsup.iloc[inconclusive_indices]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
        "if len(abnormal_subset)>0:\n",
        "  abnormal_subset.plot(legend=False, ax=ax, color=\"r\")\n",
        "if len(inconclusive_subset)>0:\n",
        "  inconclusive_subset.plot(legend=False, ax=ax, color=\"0.8\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cgb9YbZ-P27V",
      "metadata": {
        "id": "Cgb9YbZ-P27V"
      },
      "source": [
        "# NBEATS (Darts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0uWL2aLNP5KC",
      "metadata": {
        "id": "0uWL2aLNP5KC"
      },
      "outputs": [],
      "source": [
        "from darts import TimeSeries, concatenate\n",
        "from darts.utils.callbacks import TFMProgressBar\n",
        "from darts.models import NBEATSModel\n",
        "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
        "from darts.metrics import mape, r2_score\n",
        "from darts.datasets import EnergyDataset\n",
        "from darts import concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C-Ynmm6nQY7V",
      "metadata": {
        "id": "C-Ynmm6nQY7V"
      },
      "outputs": [],
      "source": [
        "def display_forecast(pred_series, ts_transformed, forecast_type, start_date=None):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    if start_date:\n",
        "        ts_transformed = ts_transformed.drop_before(start_date)\n",
        "    ts_transformed.univariate_component(0).plot(label=\"actual\")\n",
        "    pred_series.plot(label=(\"historic \" + forecast_type + \" forecasts\"))\n",
        "    plt.title(\n",
        "        \"R2: {}\".format(r2_score(ts_transformed.univariate_component(0), pred_series))\n",
        "    )\n",
        "    plt.legend()\n",
        "\n",
        "def generate_torch_kwargs():\n",
        "    # run torch models on CPU, and disable progress bars for all model stages except training.\n",
        "    return {\n",
        "        \"pl_trainer_kwargs\": {\n",
        "            \"accelerator\": \"cpu\",\n",
        "            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kFVX7HlQbaC",
      "metadata": {
        "id": "0kFVX7HlQbaC"
      },
      "outputs": [],
      "source": [
        "scaler = Scaler()\n",
        "\n",
        "darts_ts_train_orig = TimeSeries.from_series(ts_train_orig)\n",
        "darts_ts_test_orig = TimeSeries.from_series(ts_test_orig)\n",
        "\n",
        "darts_ts_train_orig_scaled = scaler.fit_transform(darts_ts_train_orig)\n",
        "darts_ts_test_orig_scaled = scaler.transform(darts_ts_test_orig)\n",
        "darts_ts_full_orig_scaled = scaler.transform(TimeSeries.from_series(solar_ts_series))\n",
        "\n",
        "darts_ts_train_orig_scaled.plot(label=\"training\")\n",
        "darts_ts_test_orig_scaled.plot(label=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08T5vQ5QS_sN",
      "metadata": {
        "id": "08T5vQ5QS_sN"
      },
      "outputs": [],
      "source": [
        "model_name = \"nbeats_run\"\n",
        "model_nbeats = NBEATSModel(\n",
        "    input_chunk_length=30,\n",
        "    output_chunk_length=7,\n",
        "    generic_architecture=True,\n",
        "    num_stacks=10,\n",
        "    num_blocks=1,\n",
        "    num_layers=4,\n",
        "    layer_widths=512,\n",
        "    n_epochs=100,\n",
        "    nr_epochs_val_period=1,\n",
        "    batch_size=800,\n",
        "    random_state=42,\n",
        "    model_name=model_name,\n",
        "    save_checkpoints=True,\n",
        "    force_reset=True,\n",
        "    **generate_torch_kwargs(),\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nkpT-YrtTHZ-",
      "metadata": {
        "id": "nkpT-YrtTHZ-"
      },
      "outputs": [],
      "source": [
        "# 5000 just for now, do not overload the poor CPU!\n",
        "model_nbeats.fit(darts_ts_train_orig_scaled[1:5000], val_series=darts_ts_test_orig_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DwTyWRp3Tpfn",
      "metadata": {
        "id": "DwTyWRp3Tpfn"
      },
      "outputs": [],
      "source": [
        "model_nbeats = NBEATSModel.load_from_checkpoint(model_name=model_name, best=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aIi5Vp-gU_-h",
      "metadata": {
        "id": "aIi5Vp-gU_-h"
      },
      "outputs": [],
      "source": [
        "# Evtl. auch was fr das FT-Modell?\n",
        "\n",
        "pred_series = model_nbeats.historical_forecasts(\n",
        "    darts_ts_full_orig_scaled,\n",
        "    start=darts_ts_test_orig_scaled.start_time(),\n",
        "    forecast_horizon=7,\n",
        "    stride=7,\n",
        "    last_points_only=False,\n",
        "    retrain=False,\n",
        "    verbose=True,\n",
        ")\n",
        "pred_series = concatenate(pred_series)\n",
        "\n",
        "display_forecast(\n",
        "    pred_series,\n",
        "    darts_ts_full_orig_scaled,\n",
        "    \"7 day\",\n",
        "    start_date=darts_ts_test_orig_scaled.start_time(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Rzr0-7mcaOs",
      "metadata": {
        "id": "8Rzr0-7mcaOs"
      },
      "source": [
        "# NHITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CxZqppS9xj9G",
      "metadata": {
        "id": "CxZqppS9xj9G"
      },
      "outputs": [],
      "source": [
        "solar_ts_series_nhits = solar_ts_series.reset_index().rename(columns={\"Datum\":\"ds\",\"Leistung\":\"y\"})\n",
        "solar_ts_series_nhits.insert(column=\"unique_id\", value=\"UID\", loc=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NwxW96CBcbN-",
      "metadata": {
        "id": "NwxW96CBcbN-"
      },
      "outputs": [],
      "source": [
        "from statsforecast import StatsForecast\n",
        "\n",
        "StatsForecast.plot(solar_ts_series_nhits, engine='matplotlib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OGyBsZfr0bvk",
      "metadata": {
        "id": "OGyBsZfr0bvk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kJqcVXGJck9Y",
      "metadata": {
        "id": "kJqcVXGJck9Y"
      },
      "outputs": [],
      "source": [
        "from ray import tune\n",
        "\n",
        "from neuralforecast.auto import AutoNHITS, AutoLSTM\n",
        "from neuralforecast.core import NeuralForecast\n",
        "\n",
        "from neuralforecast.losses.pytorch import DistributionLoss, MQLoss\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GJZULYgSco-C",
      "metadata": {
        "id": "GJZULYgSco-C"
      },
      "outputs": [],
      "source": [
        "horizon = 96 # 24hrs = 4 * 15 min.\n",
        "\n",
        "\n",
        "# Use your own config or AutoNHITS.default_config\n",
        "nhits_config = {\n",
        "    #     \"start_padding_enabled\": True,\n",
        "  # oder     \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n",
        "        \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n",
        "       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n",
        "       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n",
        "       # oder     \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n",
        "       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n",
        "#        \"batch_size\": tune.choice([1, 4, 10]),                          # Number of series in batch\n",
        "\n",
        "       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n",
        "    #     \"windows_batch_size\": tune.choice([128, 256, 512]),             # Number of windows in batch\n",
        "\n",
        "       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n",
        "       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n",
        "      # oder     \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],\n",
        "                                      #[8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n",
        "      #\"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n",
        "      #                                [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n",
        "       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n",
        "       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n",
        "      # oder     \"n_blocks\": 5*[1],                                              # Length of input window\n",
        "       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n",
        "    # oder     \"mlp_units\": 5 * [[64, 64]],                                  # Length of input window\n",
        "       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n",
        "       \"random_seed\": tune.randint(1, 10),\n",
        "       \"scaler_type\": tune.choice(['robust']),\n",
        "       \"val_check_steps\": tune.choice([100])\n",
        "    }\n",
        "\n",
        "config_lstm = {\n",
        "    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n",
        "    \"encoder_hidden_size\": tune.choice([64, 128]),            # Hidden size of LSTM cells\n",
        "    \"encoder_n_layers\": tune.choice([2,4]),                   # Number of layers in LSTM\n",
        "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),             # Initial Learning rate\n",
        "    \"scaler_type\": tune.choice(['robust']),                   # Scaler type\n",
        "    \"max_steps\": tune.choice([500, 1000]),                    # Max number of training iterations\n",
        "    \"batch_size\": tune.choice([1, 4]),                        # Number of series in batch\n",
        "    \"random_seed\": tune.randint(1, 20),                       # Random seed\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "itH6ZN76hA_w",
      "metadata": {
        "id": "itH6ZN76hA_w"
      },
      "outputs": [],
      "source": [
        "models = [AutoNHITS(h=horizon,\n",
        "                    loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n",
        "                    config=nhits_config,\n",
        "                    num_samples=5)]\n",
        "\n",
        "# oder        AutoNHITS(h=48, config=config_nhits, loss=MQLoss(), num_samples=5),\n",
        "        #AutoLSTM(h=48, config=config_lstm, loss=MQLoss(), num_samples=2),\n",
        "nf = NeuralForecast(\n",
        "    models=models,\n",
        "    freq=\"15min\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2SE8TknFo23k",
      "metadata": {
        "id": "2SE8TknFo23k"
      },
      "outputs": [],
      "source": [
        "#nf.fit(df=Y_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zJNEMVO8o3k0",
      "metadata": {
        "id": "zJNEMVO8o3k0"
      },
      "outputs": [],
      "source": [
        "#fcst_df = nf.predict()\n",
        "#fcst_df.columns = fcst_df.columns.str.replace('-median', '')\n",
        "#fcst_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zgr2HCzzqJlV",
      "metadata": {
        "id": "Zgr2HCzzqJlV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P-2SG3kko-DK",
      "metadata": {
        "id": "P-2SG3kko-DK"
      },
      "outputs": [],
      "source": [
        "# ... ooder statt fit+predict\n",
        "if False:\n",
        "  val_size = 96*10\n",
        "  test_size = 96*10\n",
        "  cv_df = nf.cross_validation(solar_ts_series_nhits, n_windows=2)\n",
        "  %%capture\n",
        "  Y_hat_df = nf.cross_validation(df=solar_ts_series_nhits, val_size=val_size,\n",
        "                                test_size=test_size, n_windows=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SknFyu2JvaB0",
      "metadata": {
        "id": "SknFyu2JvaB0"
      },
      "outputs": [],
      "source": [
        "kwitt\n",
        "Y_hat_df = Y_hat_df.reset_index(drop=True)\n",
        "Y_hat_df = Y_hat_df[(Y_hat_df['unique_id']=='OT') & (Y_hat_df['cutoff']=='2018-02-11 12:00:00')]\n",
        "Y_hat_df = Y_hat_df.drop(columns=['y','cutoff'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3hfyzLwLvc0Z",
      "metadata": {
        "id": "3hfyzLwLvc0Z"
      },
      "outputs": [],
      "source": [
        "plot_df = Y_df.merge(Y_hat_df, on=['unique_id','ds'], how='outer').tail(96*10+50+96*4).head(96*2+96*4)\n",
        "\n",
        "plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n",
        "plt.plot(plot_df['ds'], plot_df['AutoNHITS-median'], c='blue', label='median')\n",
        "plt.fill_between(x=plot_df['ds'],\n",
        "                    y1=plot_df['AutoNHITS-lo-90'], y2=plot_df['AutoNHITS-hi-90'],\n",
        "                    alpha=0.4, label='level 90')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "supzB1qMqOn3",
      "metadata": {
        "id": "supzB1qMqOn3"
      },
      "outputs": [],
      "source": [
        "cv_df.columns = cv_df.columns.str.replace('-median', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aDjaSuawqRNs",
      "metadata": {
        "id": "aDjaSuawqRNs"
      },
      "outputs": [],
      "source": [
        "cv_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2wyzpzEAqcNa",
      "metadata": {
        "id": "2wyzpzEAqcNa"
      },
      "outputs": [],
      "source": [
        "for cutoff in cv_df['cutoff'].unique():\n",
        "    StatsForecast.plot(\n",
        "        Y_df,\n",
        "        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']),\n",
        "        max_insample_length=48 * 4,\n",
        "        unique_ids=['H185'],\n",
        "        engine='matplotlib'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_oK0ODX8pO_Y",
      "metadata": {
        "id": "_oK0ODX8pO_Y"
      },
      "source": [
        "# GP1 (PACKAGE DEFUNCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-UN2l7gppOKW",
      "metadata": {
        "id": "-UN2l7gppOKW"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  # hard-coded but that's fine :)\n",
        "  from gluonts.dataset.pandas import PandasDataset\n",
        "  from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
        "\n",
        "\n",
        "  solar_ts_series_gp = solar_ts_series_nhits\n",
        "  solar_ts_series_gp\n",
        "\n",
        "  train_ds = PandasDataset.from_long_dataframe(solar_ts_series_gp, target='y', item_id='unique_id', timestamp='ds', freq='15min')\n",
        "  # I suspect that the grouper is not really necessary and that we could use train_ds directly..\n",
        "  grouper_train = MultivariateGrouper(max_target_dim=1)\n",
        "  train_ds = grouper_train(train_ds)\n",
        "\n",
        "  from gluonts.mx import Trainer, GPVAREstimator\n",
        "\n",
        "  estimator = GPVAREstimator(freq='15min', prediction_length=h, target_dim=unique_series, context_length=30, trainer = Trainer(ctx='cpu', epochs=5, learning_rate=1e-3))\n",
        "  predictor = estimator.train(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5l0607hrDnXy",
      "metadata": {
        "id": "5l0607hrDnXy"
      },
      "source": [
        "# GP2 (PACKAGE DEFUNCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AMfsni48Dnkg",
      "metadata": {
        "id": "AMfsni48Dnkg"
      },
      "outputs": [],
      "source": [
        "import gpforecaster as gpf\n",
        "import tsaugmentation as tsag\n",
        "\n",
        "# Get the data\n",
        "dataset = 'prison'\n",
        "data = tsag.preprocessing.PreprocessDatasets(dataset).apply_preprocess()\n",
        "\n",
        "# Initialize the GPHF model\n",
        "gpf_model = gpf.model.GPF(dataset, data)\n",
        "\n",
        "# Train the model\n",
        "model, like = gpf_model.train()\n",
        "\n",
        "# Plot the losses\n",
        "gpf_model.plot_losses()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aMGcWv1xwqWF",
      "metadata": {
        "id": "aMGcWv1xwqWF"
      },
      "source": [
        "#GP (Darts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QcU5tDawwtEt",
      "metadata": {
        "id": "QcU5tDawwtEt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process.kernels import ExpSineSquared, RBF\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.models import GaussianProcessFilter\n",
        "from darts.utils import timeseries_generation as tg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caAHfdtPwwEE",
      "metadata": {
        "id": "caAHfdtPwwEE"
      },
      "outputs": [],
      "source": [
        "NOISE_DISTANCE = 0.4\n",
        "SAMPLE_SIZE = 200\n",
        "np.random.seed(42)\n",
        "\n",
        "# Prepare the sine wave\n",
        "x = tg.sine_timeseries(length=SAMPLE_SIZE, value_frequency=0.025)\n",
        "\n",
        "# Add white noise\n",
        "noise = tg.gaussian_timeseries(length=SAMPLE_SIZE, std=NOISE_DISTANCE)\n",
        "x_noise = x + noise\n",
        "\n",
        "plt.figure(figsize=[12, 8])\n",
        "x.plot(label=\"Original sine wave\")\n",
        "x_noise.plot(color=\"red\", label=\"Noisy sine wave\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDClPmjPw5Nq",
      "metadata": {
        "id": "ZDClPmjPw5Nq"
      },
      "outputs": [],
      "source": [
        "kernel = ExpSineSquared()\n",
        "# kernel = RBF()\n",
        "\n",
        "gpf = GaussianProcessFilter(\n",
        "    kernel=kernel, alpha=NOISE_DISTANCE / 2, n_restarts_optimizer=100\n",
        ")\n",
        "\n",
        "filtered_x_samples = gpf.filter(x_noise, num_samples=100)\n",
        "\n",
        "plt.figure(figsize=[12, 8])\n",
        "x.plot(color=\"black\", label=\"Original sine wave\")\n",
        "x_noise.plot(color=\"red\", label=\"Noisy sine wave\")\n",
        "filtered_x_samples.plot(color=\"blue\", label=\"Confidence interval of filtered sine wave\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df253d9-7fdc-4f11-8fc5-0ba00695a399",
      "metadata": {
        "id": "4df253d9-7fdc-4f11-8fc5-0ba00695a399"
      },
      "source": [
        "# Extreme Value Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tQ-d3MUdPPo6",
      "metadata": {
        "id": "tQ-d3MUdPPo6"
      },
      "source": [
        "#GP 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vVA_Z6-5PP9n",
      "metadata": {
        "id": "vVA_Z6-5PP9n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## !!NON-ADPATED!!\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(0)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = np.sin(X).ravel()\n",
        "\n",
        "# Add noise to the data\n",
        "y += 0.1 * np.random.randn(80)\n",
        "\n",
        "# Define the kernel (RBF kernel)\n",
        "kernel = 1.0 * RBF(length_scale=1.0)\n",
        "\n",
        "# Create a Gaussian Process Regressor with the defined kernel\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "# Fit the Gaussian Process model to the training data\n",
        "gp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred, sigma = gp.predict(X_test, return_std=True)\n",
        "\n",
        "# Visualize the results\n",
        "x = np.linspace(0, 5, 1000)[:, np.newaxis]\n",
        "y_mean, y_cov = gp.predict(x, return_cov=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(X_train, y_train, c='r', label='Training Data')\n",
        "plt.plot(x, y_mean, 'k', lw=2, zorder=9, label='Predicted Mean')\n",
        "plt.fill_between(x[:, 0], y_mean - 1.96 * np.sqrt(np.diag(y_cov)), y_mean + 1.96 *\n",
        "                 np.sqrt(np.diag(y_cov)), alpha=0.2, color='k', label='95% Confidence Interval')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RGouSvGG_xBy",
      "metadata": {
        "id": "RGouSvGG_xBy"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0B3Ie9qiPeiL",
      "metadata": {
        "id": "0B3Ie9qiPeiL"
      },
      "outputs": [],
      "source": [
        "# andere Kernel:\n",
        "#kernel_ =[kernels.RBF (),\n",
        "         #kernels.RationalQuadratic(),\n",
        "         #kernels.ExpSineSquared(periodicity=10.0),\n",
        "         #kernels.DotProduct(sigma_0=1.0)**2,\n",
        "         #kernels.Matern()\n",
        "         #]\n",
        "#print(kernel_, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X6oPrgncemzU",
      "metadata": {
        "id": "X6oPrgncemzU"
      },
      "outputs": [],
      "source": [
        "for kernel in kernel_:\n",
        "\n",
        "    # Gaussian process\n",
        "\n",
        "    gp = GaussianProcessRegressor(kernel=kernel)\n",
        "\n",
        "    # Prior\n",
        "\n",
        "    x_test = np.linspace(-5, 5, n).reshape(-1, 1)\n",
        "    mu_prior, sd_prior = gp.predict(x_test, return_std=True)\n",
        "    samples_prior = gp.sample_y(x_test, 3)\n",
        "\n",
        "    # plot\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x_test, mu_prior)\n",
        "    plt.fill_between(x_test.ravel(), mu_prior - sd_prior,\n",
        "                     mu_prior + sd_prior, color='aliceblue')\n",
        "    plt.plot(x_test, samples_prior, '--')\n",
        "    plt.title('Prior')\n",
        "\n",
        "    # Fit\n",
        "\n",
        "    x_train = np.array([-4, -3, -2, -1, 1]).reshape(-1, 1)\n",
        "    y_train = np.sin(x_train)\n",
        "    gp.fit(x_train, y_train)\n",
        "\n",
        "# posterior\n",
        "\n",
        "    mu_post, sd_post = gp.predict(x_test, return_std=True)\n",
        "    mu_post = mu_post.reshape(-1)\n",
        "    samples_post = np.squeeze(gp.sample_y(x_test, 3))\n",
        "\n",
        "    # plot\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x_test, mu_post)\n",
        "    plt.fill_between(x_test.ravel(), mu_post - sd_post,\n",
        "                     mu_post + sd_post, color='aliceblue')\n",
        "    plt.plot(x_test, samples_post, '--')\n",
        "    plt.scatter(x_train, y_train, c='blue', s=50)\n",
        "    plt.title('Posterior')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(&quot;gp.kernel_&quot;, gp.kernel_)\n",
        "    print(&quot;gp.log_marginal_likelihood:&quot;,\n",
        "          gp.log_marginal_likelihood(gp.kernel_.theta))\n",
        "\n",
        "    print('-'*50, '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lgvA6PcQG9w8",
      "metadata": {
        "id": "lgvA6PcQG9w8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "mSxDqAAwEB4L",
      "metadata": {
        "id": "mSxDqAAwEB4L"
      },
      "source": [
        "# GP wie in GP forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yj4qKJRUEDFs",
      "metadata": {
        "id": "Yj4qKJRUEDFs"
      },
      "outputs": [],
      "source": [
        "import GPy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from collections.abc import Iterable\n",
        "import logging\n",
        "import collections\n",
        "\n",
        "\n",
        "class GP:\n",
        "\n",
        "    def __init__(self, frequency, period = 1, Q = 2, priors = True, restarts = 1, normalize = False, loglevel = 0):\n",
        "        self.logger = logging.getLogger(\"forgp\")\n",
        "        self.logger.setLevel(loglevel)\n",
        "        self.set_frequency(frequency)\n",
        "        self.set_period(period)\n",
        "\n",
        "        self.Q = Q\n",
        "        self.restarts = restarts\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.has_priors = priors is not False\n",
        "        if self.has_priors:\n",
        "            if priors is True:\n",
        "                print(\"gut!\")\n",
        "                self.logger.info(\"using default priors\")\n",
        "                priors = self.default_priors()\n",
        "            elif isinstance(priors, (list,pd.core.series.Series,np.ndarray)):\n",
        "                self.priors_array(priors)\n",
        "            self.init_priors(priors)\n",
        "\n",
        "\n",
        "        self.logger.info(f\"GP priors {priors} {self.has_priors}, Q={self.Q}, restarts={self.restarts}\")\n",
        "\n",
        "\n",
        "    def standard_prior(self, data):\n",
        "        \"\"\" Get the priors hash from the array of priors data\n",
        "\n",
        "        This method will name the values in the data according to the ordering used in the\n",
        "        hierarchical probabilistic programming prior estimation code.\n",
        "        This is:\n",
        "        - std devs\n",
        "        - means (variance, periodic then exp, cos for the different Qs)\n",
        "        - alpha\n",
        "        - beta\n",
        "        \"\"\"\n",
        "\n",
        "        names = [\"p_std_var\", \"p_std_other\", \"p_mu_var\", \"p_mu_periodic\"]\n",
        "        if self.Q >= 1:\n",
        "            names += [\"p_mu_exp1\", \"p_mu_cos1\"]\n",
        "        if self.Q >= 2:\n",
        "            names += [ \"p_mu_exp2\", \"p_mu_cos2\"]\n",
        "        names += [ \"p_alpha\", \"p_beta\" ]\n",
        "\n",
        "        return dict(zip(names, data))\n",
        "\n",
        "    def priors_array(self, data):\n",
        "        \"\"\" Set priors from array\n",
        "        \"\"\"\n",
        "        priors = {\n",
        "            \"p_std_var\": data[0], \"p_std_other\": data[1],\n",
        "            \"p_mu_var\": data[2], \"p_mu_rbf\": data[3],\n",
        "            \"p_mu_periodic\": data[4]\n",
        "        }\n",
        "\n",
        "        for i in range(1, self.Q+1):\n",
        "            priors[f\"p_mu_exp{i}\"] = data[5 + (i-1)*2]\n",
        "            priors[f\"p_mu_cos{i}\"] = data[6 + (i-1)*2]\n",
        "\n",
        "        self.has_priors = True\n",
        "        self.init_priors(priors)\n",
        "\n",
        "    def default_priors(self):\n",
        "        \"\"\" Get default prior values \"\"\"\n",
        "\n",
        "\n",
        "        priors = {\n",
        "                \"p_std_var\": 1.0, \"p_std_other\": 1.0,\n",
        "                \"p_mu_var\": -1.5, \"p_mu_rbf\": 1.1,\n",
        "                \"p_mu_periodic\": 0.2, \"p_mu_exp1\": -0.7, \"p_mu_cos1\": 0.5, \"p_mu_exp2\": 1.1, \"p_mu_cos2\": 1.6,\n",
        "\n",
        "            }\n",
        "\n",
        "        return priors\n",
        "\n",
        "    def init_priors(self, priors):\n",
        "        \"\"\" Initialize the prior parameters crearing the GPy priors \"\"\"\n",
        "        self.prior_var = GPy.priors.LogGaussian(priors[\"p_mu_var\"], priors[\"p_std_var\"])\n",
        "        self.prior_lscal_rbf = GPy.priors.LogGaussian(priors[\"p_mu_rbf\"], priors[\"p_std_other\"])\n",
        "        self.prior_lscal_std_periodic = GPy.priors.LogGaussian(priors[\"p_mu_periodic\"], priors[\"p_std_other\"])\n",
        "\n",
        "        if self.Q >= 1:\n",
        "            self.prior_lscal_exp_short = GPy.priors.LogGaussian(priors[\"p_mu_exp1\"], priors[\"p_std_other\"])\n",
        "            self.prior_lscal_cos_short = GPy.priors.LogGaussian(priors[\"p_mu_cos1\"], priors[\"p_std_other\"])\n",
        "\n",
        "        if self.Q == 2:\n",
        "            self.prior_lscal_exp_long = GPy.priors.LogGaussian(priors[\"p_mu_exp2\"], priors[\"p_std_other\"])\n",
        "            self.prior_lscal_cos_long = GPy.priors.LogGaussian(priors[\"p_mu_cos2\"], priors[\"p_std_other\"])\n",
        "\n",
        "\n",
        "    def set_period(self, period = 1):\n",
        "        \"\"\" Set the period of the series\n",
        "\n",
        "        Multiple expected periods can be supported by providing an array\n",
        "        \"\"\"\n",
        "\n",
        "        # check for non iterables and make an array\n",
        "        if not isinstance(period, Iterable):\n",
        "            self.periods = [ period ]\n",
        "        else:\n",
        "            self.periods = period\n",
        "\n",
        "    def set_frequency(self, frequency):\n",
        "        \"\"\" Set the data'sfrequency\n",
        "\n",
        "        The frequency can be either a standard value (monthly, quarterly, yearly, weekly, daily)\n",
        "        or a float defining the \"resolution\" of the timeseries\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            frequency : str|number\n",
        "                This can be either a standard value among: monthly, quarterly, yearly and weekly\n",
        "                or a numeric value\n",
        "        \"\"\"\n",
        "        if type(frequency) != str:\n",
        "            self.sampling_freq = frequency\n",
        "        elif frequency == 'monthly':\n",
        "            self.sampling_freq = 12\n",
        "        elif frequency == 'quarterly':\n",
        "            self.sampling_freq = 4\n",
        "        elif frequency == 'yearly':\n",
        "            self.sampling_freq = 1\n",
        "        elif frequency == 'weekly':\n",
        "            self.sampling_freq = 365.25/7.0\n",
        "        else:\n",
        "            raise Exception(f\"wrong frequency: {frequency}\")\n",
        "\n",
        "    def set_q(self, Q):\n",
        "        \"\"\" Set the number of spectral kernels (exp+cos) \"\"\"\n",
        "\n",
        "        self.Q = Q\n",
        "\n",
        "    def do_normalize(self, Y, train = True):\n",
        "        if train:\n",
        "            self.mean = np.mean(Y)\n",
        "            self.std = np.std(Y, ddof=1)\n",
        "        return (Y - self.mean) / self.std\n",
        "\n",
        "    def do_denormalize(self, Y):\n",
        "        return Y * self.std + self.mean\n",
        "\n",
        "    def build_gp(self, Yin, X = None):\n",
        "        \"\"\" Fit a gaussian process using the specified train values \"\"\"\n",
        "        use_bias = True\n",
        "\n",
        "        if X is None:\n",
        "            X = np.linspace(1/self.sampling_freq,len(Yin)/self.sampling_freq,len(Yin))\n",
        "            X = X.reshape(len(X),1)\n",
        "\n",
        "        Y = self.do_normalize(Yin, train = True) if self.normalize else Yin\n",
        "        self.Xtrain = X\n",
        "\n",
        "        #the yearly case is managed on its own.\n",
        "        lin = GPy.kern.Linear(input_dim=1)\n",
        "\n",
        "        if self.has_priors:\n",
        "            self.logger.debug(f\"Setting Variance Prior {self.prior_var}\")\n",
        "            lin.variances.set_prior(self.prior_var)\n",
        "        K = lin\n",
        "\n",
        "        if use_bias:\n",
        "            bias = GPy.kern.Bias(input_dim=1)\n",
        "            if self.has_priors:\n",
        "                self.logger.debug(f\"Setting Bias Prior {self.prior_var}\")\n",
        "                bias.variance.set_prior(self.prior_var)\n",
        "            K = K + bias\n",
        "\n",
        "        rbf = GPy.kern.RBF(input_dim=1)\n",
        "        if self.has_priors:\n",
        "            self.logger.debug(f\"Setting RBF priors var {self.prior_var} and lengthscale {self.prior_lscal_rbf}\")\n",
        "            rbf.variance.set_prior(self.prior_var)\n",
        "            rbf.lengthscale.set_prior(self.prior_lscal_rbf)\n",
        "\n",
        "        K = K + rbf\n",
        "\n",
        "        for period in self.periods:\n",
        "            #the second component  is the stdPeriodic\n",
        "            periodic = GPy.kern.StdPeriodic(input_dim=1)\n",
        "            periodic.period.fix(period) # period is set to 1 year by default\n",
        "\n",
        "            if self.has_priors:\n",
        "                self.logger.debug(f\"Setting periodic {period} lscale {self.prior_lscal_std_periodic}\")\n",
        "                periodic.lengthscale.set_prior(self.prior_lscal_std_periodic)\n",
        "                periodic.variance.set_prior(self.prior_var)\n",
        "            K = K + periodic\n",
        "\n",
        "\n",
        "        #now initiliazes the  (Q-1) SM components. Each component is rfb*cos, where\n",
        "        #the variance of the cos is set to 1.\n",
        "        for ii in range(0, self.Q):\n",
        "            cos =  GPy.kern.Cosine(input_dim=1)\n",
        "            cos.variance.fix(1)\n",
        "            rbf =  GPy.kern.RBF(input_dim=1) #input dim, variance, lenghtscale\n",
        "\n",
        "            if self.has_priors:\n",
        "                if (ii==0):\n",
        "                        rbf.variance.set_prior(self.prior_var)\n",
        "                        rbf.lengthscale.set_prior(self.prior_lscal_exp_long)\n",
        "                        cos.lengthscale.set_prior(self.prior_lscal_cos_long)\n",
        "                elif (ii==1):\n",
        "                        rbf.variance.set_prior(self.prior_var)\n",
        "                        rbf.lengthscale.set_prior(self.prior_lscal_exp_short)\n",
        "                        cos.lengthscale.set_prior(self.prior_lscal_cos_short)\n",
        "            K = K + cos * rbf\n",
        "\n",
        "\n",
        "        GPmodel = GPy.models.GPRegression(X, Y, K)\n",
        "\n",
        "        if self.has_priors:\n",
        "            GPmodel.likelihood.variance.set_prior(self.prior_var)\n",
        "\n",
        "\n",
        "        try:\n",
        "            GPmodel.optimize_restarts(self.restarts, robust=True)\n",
        "        except:\n",
        "            #in the rare case the single optimization numerically fails\n",
        "            GPmodel.optimize_restarts(5, robust=True)\n",
        "\n",
        "        self.gp_model = GPmodel\n",
        "        return GPmodel\n",
        "\n",
        "\n",
        "    def forecast(self, X_forecast):\n",
        "        if type(X_forecast) == int:\n",
        "            lastTrain = self.Xtrain[-1]\n",
        "            endTest = lastTrain + 1/self.sampling_freq * X_forecast\n",
        "            X = np.linspace(lastTrain + 1/self.sampling_freq, endTest, X_forecast)\n",
        "            X = X.reshape(len(X), 1)\n",
        "        else:\n",
        "            X = X_forecast\n",
        "\n",
        "\n",
        "        m,v = self.gp_model.predict(X)\n",
        "        s = np.sqrt(v)\n",
        "\n",
        "        upper = m + s * stats.norm.ppf(0.975)\n",
        "\n",
        "        if self.normalize:\n",
        "            return self.do_denormalize(m), self.do_denormalize(upper)\n",
        "        else:\n",
        "            return m, upper\n",
        "\n",
        "class dotdict(dict):\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "def get_col(data, name, index):\n",
        "    return data.iloc[:,index] if index is not None else data.loc[:,name]\n",
        "\n",
        "def float_arrays(data):\n",
        "    return data.str.split(\";\").apply(lambda x: np.array(x).astype(float))\n",
        "\n",
        "\n",
        "def compute_indicators(Ytest, mean, upper):\n",
        "    import properscoring as ps\n",
        "    import scipy.stats as stat\n",
        "    import numpy as np\n",
        "\n",
        "    sigma = (upper - mean)/ stat.norm.ppf(0.975)\n",
        "    fcast = mean\n",
        "\n",
        "    crps = np.zeros(len(Ytest))\n",
        "    ll = np.zeros(len(Ytest))\n",
        "\n",
        "    for jj in range(len(Ytest)):\n",
        "        crps[jj]    = ps.crps_gaussian(Ytest[jj], mu=fcast[jj], sig=sigma[jj])\n",
        "        ll[jj]      = stat.norm.logpdf(x=Ytest[jj], loc=fcast[jj], scale=sigma[jj])\n",
        "\n",
        "    mae = np.mean(np.abs(Ytest  - fcast))\n",
        "    crps = np.mean(crps)\n",
        "    ll = np.mean(ll)\n",
        "\n",
        "    return([mae, crps, ll])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KdJa0oGqQrsx",
      "metadata": {
        "id": "KdJa0oGqQrsx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FJACwxLsHDaW",
      "metadata": {
        "id": "FJACwxLsHDaW"
      },
      "outputs": [],
      "source": [
        "# asus dem gpforecaster dann die main-Funktion\n",
        "import types\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "ts_train = solar_ts_series_corrected_variance[1:10000]\n",
        "ts_test = solar_ts_series_corrected_variance[10001:11000]\n",
        "\n",
        "\n",
        "input_gp = pd.DataFrame(data={\n",
        "    \"st\": \"solar_data\", \"period\": 1/(4*24*365.25), \"mean\": 0, \"std\":1,\n",
        "    \"x\": \";\".join( [str(f) for f in ts_train] ),\n",
        "    \"xx\": \";\".join( [str(f) for f in ts_test] )}, index=[0])\n",
        "\n",
        "\n",
        "args= types.SimpleNamespace()\n",
        "args.Q = 2\n",
        "args.restarts=1\n",
        "args.default_priors = True\n",
        "args.train_col = \"x\"\n",
        "#args.xi = \"train_idx\"\n",
        "args.train_index = None\n",
        "args.test_col = \"xx\"\n",
        "args.test_index = None\n",
        "args.frequency_col = \"period\"\n",
        "args.mean_col = \"mean\"\n",
        "args.std_col = \"std\"\n",
        "#args.ti = \"test_idx\"\n",
        "# per year\n",
        "args.priors = None\n",
        "args.limit = 0\n",
        "args.frequency = 1/(4*24*365.25)\n",
        "args.normalize=False\n",
        "\n",
        "\n",
        "\n",
        "data = input_gp\n",
        "#if args.frequency.upper() != \"ANY\":\n",
        "#    data = data[data[args.frequency_col].str.upper() == args.frequency.upper()]\n",
        "#    logger.info(f\"Using only {args.frequency} data\")\n",
        "#else:\n",
        "logger.info(\"Not filterning by period\")\n",
        "\n",
        "\n",
        "\n",
        "train = get_col(data, args.train_col, args.train_index)\n",
        "test = get_col(data, args.test_col, args.test_index)\n",
        "\n",
        "train = float_arrays(train)\n",
        "test = float_arrays(test)\n",
        "\n",
        "if args.normalize:\n",
        "    means = get_col(data, args.mean_col, args.mean_index)\n",
        "    stds = get_col(data, args.std_col, args.std_index)\n",
        "\n",
        "    train = (tgprain - means) / stds\n",
        "    test = (test - means) / stds\n",
        "\n",
        "priors = None\n",
        "if args.priors is not None:\n",
        "    priors = pd.read_csv(args.priors, header=None, comment='#')\n",
        "    cut = int(len(priors) / args.priors_count)\n",
        "    priors = priors[-cut:].values.flatten()\n",
        "    pstr = str.join(\" \", priors.astype(str))\n",
        "    logger.debug(f\"Using these custom priors: {pstr}\")\n",
        "elif args.default_priors:\n",
        "    logger.info(\"Using default priors\")\n",
        "    priors = True\n",
        "else:\n",
        "    logger.info(\"Using no priors\")\n",
        "    priors = False\n",
        "\n",
        "#priors = False\n",
        "\n",
        "\n",
        "\n",
        "out = pd.DataFrame(columns=[\"st\", \"mean\", \"std\", \"center\", \"upper\"])\n",
        "for i in range(0, len(train)):\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    row = data.iloc[i,:]\n",
        "    Y = train.iloc[i]\n",
        "    if args.limit > 0:\n",
        "        Y = Y[-args.limit:]\n",
        "        logger.info(f\"train length: {len(Y)}\")\n",
        "\n",
        "    Y = Y.reshape(len(Y), 1)\n",
        "    YY = test.iloc[i]\n",
        "\n",
        "    # Stderr output to be able to identify GPy errors\n",
        "    print(f\"----------------------------------------------\", file=sys.stderr)\n",
        "    print(f\"Processing series #{i}\", file=sys.stderr)\n",
        "    print(f\"st: {row.st}\", file=sys.stderr)\n",
        "\n",
        "    logger.info(f\"Processing series #{i}\")\n",
        "    logger.info(f\"st: {row.st}\")\n",
        "    logger.debug(f\"{row[args.frequency_col]}\")\n",
        "    logger.debug(f\"series mean: {row[args.mean_col]}\")\n",
        "    logger.debug(f\"series std: {row[args.std_col]}\")\n",
        "    logger.debug(f\"train length: {len(Y)}/{len(train.iloc[i])}\")\n",
        "    logger.debug(f\"test length: {len(YY)}\")\n",
        "\n",
        "    g = GP(row[args.frequency_col], priors=priors, Q=args.Q, normalize=False, restarts=args.restarts)\n",
        "    if type(priors) == \"list\":\n",
        "        g.priors_array(priors)\n",
        "\n",
        "    g.build_gp(Y)\n",
        "\n",
        "    m, u = g.forecast(len(YY))\n",
        "    m = m.reshape(len(m))\n",
        "    u = u.reshape(len(u))\n",
        "\n",
        "    mae, crps, ll = compute_indicators(YY, m, u)\n",
        "    end = time.time()\n",
        "    logger.debug(f\"duration: {end - start}\")\n",
        "\n",
        "    out = out.append([{\n",
        "        \"st\":row.st,\n",
        "        \"mean\":row[args.mean_col],\n",
        "        \"std\":row[args.std_col],\n",
        "        \"time\": end - start,\n",
        "        \"center\":str.join(\";\", m.astype(str)),\n",
        "        \"upper\":str.join(\";\", u.astype(str)),\n",
        "        \"mae\": mae,\n",
        "        \"crps\": crps,\n",
        "        \"ll\": ll\n",
        "    }])\n",
        "\n",
        "out.to_csv(args.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OjpQIrsoNvH6",
      "metadata": {
        "id": "OjpQIrsoNvH6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J2PKPRuqN0sV",
      "metadata": {
        "id": "J2PKPRuqN0sV"
      },
      "outputs": [],
      "source": [
        "args.test_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UXI5pC7wKVhs",
      "metadata": {
        "id": "UXI5pC7wKVhs"
      },
      "outputs": [],
      "source": [
        "ts_train_gp.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PSyHOqLPJCi8",
      "metadata": {
        "id": "PSyHOqLPJCi8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f28f4bc4-ad73-4624-8d29-e2a79de258ab",
      "metadata": {
        "id": "f28f4bc4-ad73-4624-8d29-e2a79de258ab"
      },
      "source": [
        "# Gaussian Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ddbd41-dd41-411b-abeb-1ddaa2a00c31",
      "metadata": {
        "id": "95ddbd41-dd41-411b-abeb-1ddaa2a00c31"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4349da86-b381-4a65-b588-c6359adc8722",
      "metadata": {
        "id": "4349da86-b381-4a65-b588-c6359adc8722"
      },
      "outputs": [],
      "source": [
        "train_size = int(len(solar_ts_series_corrected_variance) * 0.7)\n",
        "ts_train = solar_ts_series_corrected_variance[:train_size]\n",
        "ts_test = solar_ts_series_corrected_variance[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1da0da-6b5f-4bd7-9fc1-c686367568fb",
      "metadata": {
        "id": "dd1da0da-6b5f-4bd7-9fc1-c686367568fb"
      },
      "outputs": [],
      "source": [
        "# oder, fr einfache arrays x, y:# Split the data into training and testing sets\n",
        "#X_train, X_test, y_train, y_test = train_test_split(\n",
        "#    X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "X_train = ts_train.values.reshape(-1,1)\n",
        "Y_train = ts_test.values.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9c998c-e5dd-4562-8810-32af1ba26d9d",
      "metadata": {
        "id": "fb9c998c-e5dd-4562-8810-32af1ba26d9d"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1)\n",
        "model = GaussianProcessRegressor(kernel=kernel)\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred, sigma = gp.predict(X_test, return_std=True)\n",
        "\n",
        "# Visualize the results\n",
        "x = np.linspace(0, 5, 1000)[:, np.newaxis]\n",
        "y_mean, y_cov = gp.predict(x, return_cov=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(X_train, y_train, c='r', label='Training Data')\n",
        "plt.plot(x, y_mean, 'k', lw=2, zorder=9, label='Predicted Mean')\n",
        "plt.fill_between(x[:, 0], y_mean - 1.96 * np.sqrt(np.diag(y_cov)), y_mean + 1.96 *\n",
        "                 np.sqrt(np.diag(y_cov)), alpha=0.2, color='k', label='95% Confidence Interval')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5182471-9553-4f2d-800e-8481f34e566c",
      "metadata": {
        "id": "f5182471-9553-4f2d-800e-8481f34e566c"
      },
      "outputs": [],
      "source": [
        "# end of training data: x_t. Before: window, after: horizon\n",
        "# \"predicting a number\" = regression problem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NidqZ3nj_XoR",
      "metadata": {
        "id": "NidqZ3nj_XoR"
      },
      "source": [
        "# EVT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f-NoG3Ss_ZuD",
      "metadata": {
        "id": "f-NoG3Ss_ZuD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe39546a-e158-459c-b3c3-64cb2ec9631e",
      "metadata": {
        "id": "fe39546a-e158-459c-b3c3-64cb2ec9631e"
      },
      "outputs": [],
      "source": [
        "kwitt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1084672e-2558-4252-8beb-49fd3a23e5bf",
      "metadata": {
        "id": "1084672e-2558-4252-8beb-49fd3a23e5bf"
      },
      "outputs": [],
      "source": [
        "from pyextremes import __version__, get_extremes\n",
        "from pyextremes.plotting import plot_extremes\n",
        "from pyextremes import EVA\n",
        "print(\"pyextremes\", __version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a841ad-fcef-4a92-85cb-335410cc0945",
      "metadata": {
        "id": "e9a841ad-fcef-4a92-85cb-335410cc0945"
      },
      "outputs": [],
      "source": [
        "# \"In order for the analysis results to be meaningful, data needs to be pre-processed by the user.\n",
        "# This may include removal of data gaps, detrending, interpolation, removal of outliers, etc.\"\n",
        "# ==> !! Data needs to be detrended! TODO: Does this also imply constant variance?\n",
        "# I assume yes and take solar_ts_series_corrected_variance as in input\n",
        "# TODO: So what exactly does pyextremes expect from the time series?\n",
        "from pyextremes import EVA\n",
        "\n",
        "solar_ts_series_corrected_variance\n",
        "\n",
        "model = EVA(solar_ts_series_corrected_variance)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1554d6-43ca-41d7-8bf1-63694958fdbb",
      "metadata": {
        "id": "6a1554d6-43ca-41d7-8bf1-63694958fdbb"
      },
      "outputs": [],
      "source": [
        "model.get_extremes(method=\"BM\", block_size=\"365.2425D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d4d023-2c28-4b3d-b681-4d12127973b3",
      "metadata": {
        "id": "79d4d023-2c28-4b3d-b681-4d12127973b3"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96daa35-5785-4a9e-ad45-f2d62b1ab872",
      "metadata": {
        "id": "c96daa35-5785-4a9e-ad45-f2d62b1ab872"
      },
      "outputs": [],
      "source": [
        "model.fit_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414d473d-d9ef-4dea-94b2-5bf1217ef55a",
      "metadata": {
        "id": "414d473d-d9ef-4dea-94b2-5bf1217ef55a"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21bd7c35-a112-4d7d-8704-26bb6c417452",
      "metadata": {
        "id": "21bd7c35-a112-4d7d-8704-26bb6c417452"
      },
      "outputs": [],
      "source": [
        "summary = model.get_summary(\n",
        "    return_period=[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000],\n",
        "    alpha=0.95,\n",
        "    n_samples=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e49f403-f971-420a-a468-4fe6c2ab8f5b",
      "metadata": {
        "id": "3e49f403-f971-420a-a468-4fe6c2ab8f5b"
      },
      "outputs": [],
      "source": [
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8262360c-8f72-446b-be03-cce5284b1c44",
      "metadata": {
        "id": "8262360c-8f72-446b-be03-cce5284b1c44"
      },
      "outputs": [],
      "source": [
        "model.plot_diagnostic(alpha=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30240b44-401b-46be-bec0-64293108636c",
      "metadata": {
        "id": "30240b44-401b-46be-bec0-64293108636c"
      },
      "outputs": [],
      "source": [
        "extremes = get_extremes(\n",
        "    ts=solar_ts_series_corrected_variance,\n",
        "    method=\"BM\",\n",
        "    extremes_type=\"high\",\n",
        "    block_size=\"365.2425D\",\n",
        "    errors=\"raise\",\n",
        "    min_last_block=None,\n",
        ")\n",
        "\n",
        "plot_extremes(\n",
        "    ts=solar_ts_series_corrected_variance,\n",
        "    extremes=extremes,\n",
        "    extremes_method=\"BM\",\n",
        "    extremes_type=\"low\",\n",
        "    block_size=\"365.2425D\",\n",
        ")\n",
        "extremes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275c4bd2-47d0-4c83-b2ee-eb1cc52d7834",
      "metadata": {
        "id": "275c4bd2-47d0-4c83-b2ee-eb1cc52d7834"
      },
      "outputs": [],
      "source": [
        "model.get_extremes(\"POT\", threshold=3, r=\"12H\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84348feb-73e0-4b31-8f50-8797a8cd68b3",
      "metadata": {
        "id": "84348feb-73e0-4b31-8f50-8797a8cd68b3"
      },
      "outputs": [],
      "source": [
        "model.plot_extremes(show_clusters=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6da094e-d5b3-44af-b39c-3bdd05fe6ed5",
      "metadata": {
        "id": "d6da094e-d5b3-44af-b39c-3bdd05fe6ed5"
      },
      "outputs": [],
      "source": [
        "# evtl. ntzlich um Modelle zu vergleichen\n",
        "\n",
        "def evaluate_preds(y_true, y_pred):\n",
        "  # Make sure float32 (for metric calculations)\n",
        "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "  # Calculate various metrics\n",
        "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
        "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
        "  rmse = tf.sqrt(mse)\n",
        "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "  return {\"mae\": mae.numpy(),\n",
        "          \"mse\": mse.numpy(),\n",
        "          \"rmse\": rmse.numpy(),\n",
        "          \"mape\": mape.numpy(),\n",
        "          \"mase\": mase.numpy()}\n",
        "\n",
        "\n",
        "# consider using the aggregating version (which loses information though - do we really want that?)\n",
        "def evaluate_preds(y_true, y_pred):\n",
        "  # Make sure float32 (for metric calculations)\n",
        "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "  # Calculate various metrics\n",
        "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
        "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
        "  rmse = tf.sqrt(mse)\n",
        "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "  # Account for different sized metrics (for longer horizons, reduce to single number)\n",
        "  if mae.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n",
        "    mae = tf.reduce_mean(mae)\n",
        "    mse = tf.reduce_mean(mse)\n",
        "    rmse = tf.reduce_mean(rmse)\n",
        "    mape = tf.reduce_mean(mape)\n",
        "    mase = tf.reduce_mean(mase)\n",
        "\n",
        "  return {\"mae\": mae.numpy(),\n",
        "          \"mse\": mse.numpy(),\n",
        "          \"rmse\": rmse.numpy(),\n",
        "          \"mape\": mape.numpy(),\n",
        "          \"mase\": mase.numpy()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad9d055-1cbf-41a0-995b-6d8035be8a9b",
      "metadata": {
        "id": "aad9d055-1cbf-41a0-995b-6d8035be8a9b"
      },
      "outputs": [],
      "source": [
        "HORIZON = 1 # predict 1 step at a time\n",
        "WINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon\n",
        "\n",
        "# Create function to label windowed data\n",
        "def get_labelled_windows(x, horizon=1):\n",
        "  \"\"\"\n",
        "  Creates labels for windowed dataset.\n",
        "\n",
        "  E.g. if horizon=1 (default)\n",
        "  Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n",
        "  \"\"\"\n",
        "  return x[:, :-horizon], x[:, -horizon:]\n",
        "\n",
        "\n",
        "# Test out the window labelling function\n",
        "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\n",
        "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")\n",
        "\n",
        "# Create function to view NumPy arrays as windows\n",
        "def make_windows(x, window_size=7, horizon=1):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
        "  \"\"\"\n",
        "  # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  # print(f\"Window step:\\n {window_step}\")\n",
        "\n",
        "  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
        "  # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
        "\n",
        "  # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
        "  windowed_array = x[window_indexes]\n",
        "\n",
        "  # 4. Get the labelled windows\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "  return windows, labels\n",
        "\n",
        "\n",
        "# Consider using tf.keras.preprocessing.timeseries_dataset_from_array() instead!\n",
        "\n",
        "# Make the train/test splits\n",
        "def make_train_test_splits(windows, labels, test_split=0.2):\n",
        "  \"\"\"\n",
        "  Splits matching pairs of windows and labels into train and test splits.\n",
        "  \"\"\"\n",
        "  split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test\n",
        "  train_windows = windows[:split_size]\n",
        "  train_labels = labels[:split_size]\n",
        "  test_windows = windows[split_size:]\n",
        "  test_labels = labels[split_size:]\n",
        "  return train_windows, test_windows, train_labels, test_labels\n",
        "\n",
        "import os\n",
        "\n",
        "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
        "                                            verbose=0, # only output a limited amount of text\n",
        "                                            save_best_only=True) # save only the best model to file\n",
        "\n",
        "def make_preds(model, input_data):\n",
        "  \"\"\"\n",
        "  Uses model to make predictions on input_data.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model: trained model\n",
        "  input_data: windowed input data (same kind of data model was trained on)\n",
        "\n",
        "  Returns model predictions on input_data.\n",
        "  \"\"\"\n",
        "  forecast = model.predict(input_data)\n",
        "  return tf.squeeze(forecast) # return 1D array of predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f685cd-6134-4336-91ef-26b9a66eef4d",
      "metadata": {
        "id": "b2f685cd-6134-4336-91ef-26b9a66eef4d"
      },
      "outputs": [],
      "source": [
        "# optional (dense)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Set random seed for as reproducible results as possible\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Construct model\n",
        "model_1 = tf.keras.Sequential([\n",
        "  layers.Dense(128, activation=\"relu\"),\n",
        "  layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation\n",
        "], name=\"model_1_dense\") # give the model a name so we can save it\n",
        "\n",
        "# Compile model\n",
        "model_1.compile(loss=\"mae\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE\n",
        "\n",
        "# Fit model\n",
        "model_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices\n",
        "            y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)\n",
        "            epochs=100,\n",
        "            verbose=1,\n",
        "            batch_size=128,\n",
        "            validation_data=(test_windows, test_labels),\n",
        "            callbacks=[create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoint callback to save bes\n",
        "\n",
        "# maybe consider using EarlyStopping instead?\n",
        "# Load in saved best performing model_1 and evaluate on test data\n",
        "model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\")\n",
        "model_1.evaluate(test_windows, test_labels)\n",
        "\n",
        "# Make predictions using model_1 on the test dataset and view the results\n",
        "model_1_preds = make_preds(model_1, test_windows)\n",
        "len(model_1_preds), model_1_preds[:10]\n",
        "\n",
        "# Evaluate preds\n",
        "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape\n",
        "                                 y_pred=model_1_preds)\n",
        "model_1_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166be90d-1a3e-4f66-a7b1-fa559e8791ff",
      "metadata": {
        "id": "166be90d-1a3e-4f66-a7b1-fa559e8791ff"
      },
      "outputs": [],
      "source": [
        "# model 2: use different WINDOW_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb93f794-5e38-452c-85c8-01eceaa53f70",
      "metadata": {
        "id": "eb93f794-5e38-452c-85c8-01eceaa53f70"
      },
      "outputs": [],
      "source": [
        "# model 3: horizon > 1 ==> N nodes in the last dense layer, unrolled by make_preds (supposedly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7231da4d-b18a-4381-90e2-53b05b5588a2",
      "metadata": {
        "id": "7231da4d-b18a-4381-90e2-53b05b5588a2"
      },
      "outputs": [],
      "source": [
        "# NBEATS\n",
        "# Create NBeatsBlock custom layer\n",
        "class NBeatsBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, # the constructor takes all the hyperparameters for the layer\n",
        "               input_size: int,\n",
        "               theta_size: int,\n",
        "               horizon: int,\n",
        "               n_neurons: int,\n",
        "               n_layers: int,\n",
        "               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n",
        "    super().__init__(**kwargs)\n",
        "    self.input_size = input_size\n",
        "    self.theta_size = theta_size\n",
        "    self.horizon = horizon\n",
        "    self.n_neurons = n_neurons\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Block contains stack of 4 fully connected layers each has ReLU activation\n",
        "    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n",
        "    # Output of block is a theta layer with linear activation\n",
        "    self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n",
        "\n",
        "  def call(self, inputs): # the call method is what runs when the layer is called\n",
        "    x = inputs\n",
        "    for layer in self.hidden: # pass inputs through each hidden layer\n",
        "      x = layer(x)\n",
        "    theta = self.theta_layer(x)\n",
        "    # Output the backcast and forecast from theta\n",
        "    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n",
        "    return backcast, forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078d35fc-f946-4795-bc0e-86a413ab6267",
      "metadata": {
        "id": "078d35fc-f946-4795-bc0e-86a413ab6267"
      },
      "outputs": [],
      "source": [
        "ts_train.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7294675e-addb-4aa1-bbaa-a2351061ce74",
      "metadata": {
        "id": "7294675e-addb-4aa1-bbaa-a2351061ce74"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9075914-c361-4867-a743-feab006e118e",
      "metadata": {
        "id": "a9075914-c361-4867-a743-feab006e118e"
      },
      "outputs": [],
      "source": [
        "#train_size = int(len(solar_ts_series_corrected_variance) * 0.7)\n",
        "#ts_train = solar_ts_series_corrected_variance[:train_size]\n",
        "#ts_test = solar_ts_series_corrected_variance[train_size:]\n",
        "\n",
        "# Add windowed columns\n",
        "watts_nbeats = solar_ts_series_corrected_variance.to_frame(\"Leistung\")\n",
        "for i in range(WINDOW_SIZE):\n",
        "  watts_nbeats[f\"Leistung+{i+1}\"] = watts_nbeats[\"Leistung\"].shift(periods=i+1)\n",
        "watts_nbeats.dropna().head()\n",
        "\n",
        "X = watts_nbeats.dropna().drop(\"Leistung\", axis=1)\n",
        "y = watts_nbeats.dropna()[\"Leistung\"]\n",
        "\n",
        "# Make train and test sets\n",
        "split_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:split_size], y[:split_size]\n",
        "X_test, y_test = X[split_size:], y[split_size:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)\n",
        "\n",
        "\n",
        "\n",
        "#X_train = solar_ts_series_corrected_variance.index.to_numpy()\n",
        "#X_train = np.array([x.to_pydatetime() for x in X_train]).astype('datetime64[ns]')\n",
        "#X_test = solar_ts_series_corrected_variance.index.to_numpy()\n",
        "#X_test = np.array([x.to_pydatetime() for x in X_test]).astype('datetime64[ns]')\n",
        "#Y_train = ts_train.values.reshape(-1,1)\n",
        "#Y_test = ts_test.values.reshape(-1,1)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Turn train and test arrays into tensor Datasets\n",
        "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "train_labels_dataset = tf.data.Dataset.from_tensor_slices(Y_train)\n",
        "\n",
        "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "# 2. Combine features & labels\n",
        "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
        "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
        "\n",
        "# 3. Batch and prefetch for optimal performance\n",
        "BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b55284e-00e1-469e-b3c5-fa4a6d54d58d",
      "metadata": {
        "id": "3b55284e-00e1-469e-b3c5-fa4a6d54d58d"
      },
      "outputs": [],
      "source": [
        "watts_nbeats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b37a7a4-901f-4427-97f9-89a69668a439",
      "metadata": {
        "id": "3b37a7a4-901f-4427-97f9-89a69668a439"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e09ecac-7e9d-4f8e-bf61-78233ed1f384",
      "metadata": {
        "id": "0e09ecac-7e9d-4f8e-bf61-78233ed1f384"
      },
      "outputs": [],
      "source": [
        "dt_object = datetime.fromtimestamp(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83438a9c-7ad8-4250-95a5-8b90210f1891",
      "metadata": {
        "id": "83438a9c-7ad8-4250-95a5-8b90210f1891"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892b442e-fef4-4a25-b4c8-7b9b81f3d9ce",
      "metadata": {
        "id": "892b442e-fef4-4a25-b4c8-7b9b81f3d9ce"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0410b6",
      "metadata": {
        "id": "cc0410b6"
      },
      "source": [
        "# Naive Model and Backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec4f289",
      "metadata": {
        "id": "dec4f289"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe5c553",
      "metadata": {
        "id": "dfe5c553"
      },
      "outputs": [],
      "source": [
        "# Data Preparation for Naive Model\n",
        "ts = solar_ts_series_corrected_variance\n",
        "ts = ts.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1712f878",
      "metadata": {
        "id": "1712f878"
      },
      "outputs": [],
      "source": [
        "# Moving average model\n",
        "def moving_average(data: pd.DataFrame, window_size: int=4*3, shift_size: int=96):\n",
        "    moving_avg = data.rolling(window=window_size).mean()\n",
        "    shifted_moving_avg = moving_avg.shift(shift_size)\n",
        "    return(shifted_moving_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a5fc6f0",
      "metadata": {
        "id": "0a5fc6f0"
      },
      "outputs": [],
      "source": [
        "# Plot Naive Model\n",
        "naive_model = moving_average(ts)\n",
        "\n",
        "test_date_start = '2024-01-01 00:00+00:00'\n",
        "test_ts = ts[test_date_start:]\n",
        "naive_model_print = naive_model[test_date_start:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_ts.index, test_ts, label='Original')\n",
        "plt.plot(naive_model_print.index, naive_model_print, label='Moving average', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Naive Model v.s. Original Data')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Time Series')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795ce79d",
      "metadata": {
        "id": "795ce79d"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesPredictionModel():\n",
        "    \"\"\"\n",
        "    Time series prediction model implementation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        model_name : class\n",
        "            Choice of regressor\n",
        "        model_params : dict\n",
        "            Definition of model specific tuning parameters\n",
        "\n",
        "    Functions\n",
        "    ----------\n",
        "        train : Train chosen model\n",
        "        forcast : Apply trained model to prediction period and generate forecast DataFrame\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name,\n",
        "                 model_params: dict) -> None:\n",
        "        \"\"\"Initialize a new instance of time_series_prediction_model.\"\"\"\n",
        "        self.model = model_name(**model_params)\n",
        "\n",
        "    def train(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
        "        \"\"\"Train chosen model.\"\"\"\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "    def forecast(self, X_test: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Apply trained model to prediction period and generate forecast DataFrame.\"\"\"\n",
        "        self.X_test = X_test\n",
        "        forecast_df = pd.DataFrame(self.model.predict(self.X_test), index=self.X_test.index)\n",
        "        forecast_df.index.name = 'Datum'\n",
        "        return forecast_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2858dc0",
      "metadata": {
        "id": "f2858dc0"
      },
      "outputs": [],
      "source": [
        "# Dummy data preparation for TimeSeriesPredictionModel\n",
        "\n",
        "data = pd.DataFrame(index=ts.index, columns=['25h_lag', '24h_lag', 'Original'])\n",
        "data['Original'] = ts\n",
        "data['24h_lag'] = ts.shift(96)\n",
        "data['25h_lag'] = ts.shift(100)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f82045",
      "metadata": {
        "id": "16f82045"
      },
      "outputs": [],
      "source": [
        "test_date_start = '2024-01-01 00:00+00:00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b731674e",
      "metadata": {
        "id": "b731674e"
      },
      "outputs": [],
      "source": [
        "# Dummy Data train-test split\n",
        "train_df = data[:test_date_start]\n",
        "train_df = train_df.drop(train_df.tail(1).index)\n",
        "X_train = train_df[['25h_lag', '24h_lag']]\n",
        "y_train = train_df[['Original']]\n",
        "\n",
        "test_df = data[test_date_start:]\n",
        "X_test = test_df[['25h_lag', '24h_lag']]\n",
        "y_test = test_df[['Original']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3330d3ca",
      "metadata": {
        "id": "3330d3ca"
      },
      "outputs": [],
      "source": [
        "# Backtesting mit Sliding Window\n",
        "\n",
        "def backtesting(X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
        "                X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
        "                model: TimeSeriesPredictionModel, prediction_step_size: int=96):\n",
        "\n",
        "    # initializing output df\n",
        "    predictions = pd.DataFrame(index=y_test.index, columns=['Original', 'Predictions'])\n",
        "    predictions['Original'] = y_test\n",
        "\n",
        "    for i in range(0, len(X_test)-prediction_step_size, prediction_step_size):\n",
        "        end_idx = i + prediction_step_size\n",
        "        forecast_index= X_test.iloc[i:end_idx].index\n",
        "\n",
        "        # fit model and predict\n",
        "        model.train(X_train, y_train)\n",
        "        forecast = model.forecast(X_test.iloc[i:end_idx])\n",
        "        predictions.loc[forecast_index, 'Predictions'] = forecast.to_numpy()\n",
        "\n",
        "        print(f'Finished Forecast for {forecast_index[-1].date()}')\n",
        "\n",
        "        # delete old time window from train data\n",
        "        X_train = X_train.drop(X_train.head(prediction_step_size).index)\n",
        "        y_train = y_train.drop(y_train.head(prediction_step_size).index)\n",
        "\n",
        "        # add next time window to train data\n",
        "        X_train = pd.concat([X_train, X_test.iloc[i:end_idx]])\n",
        "        y_train = pd.concat([y_train, y_test.iloc[i:end_idx]])\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be653365",
      "metadata": {
        "id": "be653365"
      },
      "outputs": [],
      "source": [
        "# Initializing random forest regressor as instance of TimeSeriesPredictionModel\n",
        "rdnf = TimeSeriesPredictionModel(RandomForestRegressor, {'n_estimators': 10, 'criterion': 'squared_error', 'max_depth': 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c31d415",
      "metadata": {
        "id": "9c31d415"
      },
      "outputs": [],
      "source": [
        "rdnf_pred = backtesting(X_train, y_train, X_test, y_test, rdnf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ecffe4",
      "metadata": {
        "id": "c3ecffe4"
      },
      "outputs": [],
      "source": [
        "rdnf_pred = rdnf_pred.dropna() # ausgehend vom ersten Testzeitraum werden nur vollstndige Test-Perioden predicted\n",
        "rdnf_pred.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac37442",
      "metadata": {
        "id": "7ac37442"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "\n",
        "def evaluation(y_true, y_pred):\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    return mae, mape, mse, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c607956",
      "metadata": {
        "id": "3c607956"
      },
      "outputs": [],
      "source": [
        "mae, mape, mse, r2 = evaluation(rdnf_pred['Original'], rdnf_pred['Predictions'])\n",
        "\n",
        "print(f'Model: Random Forest \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2434ac7e",
      "metadata": {
        "id": "2434ac7e"
      },
      "outputs": [],
      "source": [
        "mae, mape, mse, r2 = evaluation(test_ts, naive_model_print)\n",
        "\n",
        "print(f'Model: Naive Moving Average \\n Mean absolute error: {mae}\\n Mean absolute percentage error: {mape} \\n Mean squared error: {mse} \\n r2_score: {r2}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}